No devices were found
Setting seed -  3
Creating multiple envs -  4
---------------------------------
Environment created
Box(-1.0, 1.0, (2,), float32) Box(-inf, inf, (8,), float64)
Loading Initial saved model
Model loaded
Starting evaluation
244
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 345.434445272185
avg cum rews: 345.434445272185, std: 0.0
the best agent: 0, best agent cum rewards: 345.434445272185
254
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 346.57547024439214
avg cum rews: 346.57547024439214, std: 0.0
the best agent: 0, best agent cum rewards: 346.57547024439214
264
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 348.6790655598517
avg cum rews: 348.6790655598517, std: 0.0
the best agent: 0, best agent cum rewards: 348.6790655598517
274
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 345.7465961144414
avg cum rews: 345.7465961144414, std: 0.0
the best agent: 0, best agent cum rewards: 345.7465961144414
284
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 345.4658768079016
avg cum rews: 345.4658768079016, std: 0.0
the best agent: 0, best agent cum rewards: 345.4658768079016
294
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 352.9358219141472
avg cum rews: 352.9358219141472, std: 0.0
the best agent: 0, best agent cum rewards: 352.9358219141472
304
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 354.2682481735698
avg cum rews: 354.2682481735698, std: 0.0
the best agent: 0, best agent cum rewards: 354.2682481735698
314
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 351.8955845632816
avg cum rews: 351.8955845632816, std: 0.0
the best agent: 0, best agent cum rewards: 351.8955845632816
324
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 350.71360889731477
avg cum rews: 350.71360889731477, std: 0.0
the best agent: 0, best agent cum rewards: 350.71360889731477
334
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 358.1135832898463
avg cum rews: 358.1135832898463, std: 0.0
the best agent: 0, best agent cum rewards: 358.1135832898463
344
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 356.7660765874945
avg cum rews: 356.7660765874945, std: 0.0
the best agent: 0, best agent cum rewards: 356.7660765874945
354
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 357.96950217027296
avg cum rews: 357.96950217027296, std: 0.0
the best agent: 0, best agent cum rewards: 357.96950217027296
364
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 356.14806736990903
avg cum rews: 356.14806736990903, std: 0.0
the best agent: 0, best agent cum rewards: 356.14806736990903
374
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 360.33765342480865
avg cum rews: 360.33765342480865, std: 0.0
the best agent: 0, best agent cum rewards: 360.33765342480865
384
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 357.1383276762317
avg cum rews: 357.1383276762317, std: 0.0
the best agent: 0, best agent cum rewards: 357.1383276762317
394
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 358.03870894471453
avg cum rews: 358.03870894471453, std: 0.0
the best agent: 0, best agent cum rewards: 358.03870894471453
404
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 356.88171328810705
avg cum rews: 356.88171328810705, std: 0.0
the best agent: 0, best agent cum rewards: 356.88171328810705
414
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 356.39171923704225
avg cum rews: 356.39171923704225, std: 0.0
the best agent: 0, best agent cum rewards: 356.39171923704225
424
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 360.40436701486465
avg cum rews: 360.40436701486465, std: 0.0
the best agent: 0, best agent cum rewards: 360.40436701486465
434
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 356.7736109538303
avg cum rews: 356.7736109538303, std: 0.0
the best agent: 0, best agent cum rewards: 356.7736109538303
444
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 359.68865888738014
avg cum rews: 359.68865888738014, std: 0.0
the best agent: 0, best agent cum rewards: 359.68865888738014
454
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 358.2415719729691
avg cum rews: 358.2415719729691, std: 0.0
the best agent: 0, best agent cum rewards: 358.2415719729691
464
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 357.2195673203411
avg cum rews: 357.2195673203411, std: 0.0
the best agent: 0, best agent cum rewards: 357.2195673203411
474
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 357.534697669895
avg cum rews: 357.534697669895, std: 0.0
the best agent: 0, best agent cum rewards: 357.534697669895
484
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 361.1652946112527
avg cum rews: 361.1652946112527, std: 0.0
the best agent: 0, best agent cum rewards: 361.1652946112527
494
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 357.2272475359826
avg cum rews: 357.2272475359826, std: 0.0
the best agent: 0, best agent cum rewards: 357.2272475359826
504
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 358.2106839940099
avg cum rews: 358.2106839940099, std: 0.0
the best agent: 0, best agent cum rewards: 358.2106839940099
514
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 358.2912118516194
avg cum rews: 358.2912118516194, std: 0.0
the best agent: 0, best agent cum rewards: 358.2912118516194
524
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 359.6365389662746
avg cum rews: 359.6365389662746, std: 0.0
the best agent: 0, best agent cum rewards: 359.6365389662746
534
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 360.9471805251112
avg cum rews: 360.9471805251112, std: 0.0
the best agent: 0, best agent cum rewards: 360.9471805251112
544
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 359.4022311519316
avg cum rews: 359.4022311519316, std: 0.0
the best agent: 0, best agent cum rewards: 359.4022311519316
554
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 362.0787973567997
avg cum rews: 362.0787973567997, std: 0.0
the best agent: 0, best agent cum rewards: 362.0787973567997
564
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 359.2077036645878
avg cum rews: 359.2077036645878, std: 0.0
the best agent: 0, best agent cum rewards: 359.2077036645878
574
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 360.6982278473154
avg cum rews: 360.6982278473154, std: 0.0
the best agent: 0, best agent cum rewards: 360.6982278473154
584
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 359.8015730140741
avg cum rews: 359.8015730140741, std: 0.0
the best agent: 0, best agent cum rewards: 359.8015730140741
594
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 358.4829880632349
avg cum rews: 358.4829880632349, std: 0.0
the best agent: 0, best agent cum rewards: 358.4829880632349
604
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 360.3304320059279
avg cum rews: 360.3304320059279, std: 0.0
the best agent: 0, best agent cum rewards: 360.3304320059279
614
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 360.943129785419
avg cum rews: 360.943129785419, std: 0.0
the best agent: 0, best agent cum rewards: 360.943129785419
624
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 358.96946145184364
avg cum rews: 358.96946145184364, std: 0.0
the best agent: 0, best agent cum rewards: 358.96946145184364
634
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 359.40304637798937
avg cum rews: 359.40304637798937, std: 0.0
the best agent: 0, best agent cum rewards: 359.40304637798937
644
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 359.2482589261453
avg cum rews: 359.2482589261453, std: 0.0
the best agent: 0, best agent cum rewards: 359.2482589261453
654
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 358.1234606436169
avg cum rews: 358.1234606436169, std: 0.0
the best agent: 0, best agent cum rewards: 358.1234606436169
664
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 360.44484063871465
avg cum rews: 360.44484063871465, std: 0.0
the best agent: 0, best agent cum rewards: 360.44484063871465
674
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 361.7115072871524
avg cum rews: 361.7115072871524, std: 0.0
the best agent: 0, best agent cum rewards: 361.7115072871524
684
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 360.91390248330634
avg cum rews: 360.91390248330634, std: 0.0
the best agent: 0, best agent cum rewards: 360.91390248330634
694
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 359.00583007065165
avg cum rews: 359.00583007065165, std: 0.0
the best agent: 0, best agent cum rewards: 359.00583007065165
704
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 362.48175703403086
avg cum rews: 362.48175703403086, std: 0.0
the best agent: 0, best agent cum rewards: 362.48175703403086
714
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 362.43086704341573
avg cum rews: 362.43086704341573, std: 0.0
the best agent: 0, best agent cum rewards: 362.43086704341573
724
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: 363.6204934387011
avg cum rews: 363.6204934387011, std: 0.0
the best agent: 0, best agent cum rewards: 363.6204934387011
Average distance of random agents to nearest neighbors: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Time taken for each iteration: [48.646137714385986, 96.61303877830505, 144.7772581577301, 193.12305068969727, 241.2343738079071, 289.493469953537, 337.7621216773987, 385.6471264362335, 433.9134199619293, 481.9346010684967, 529.6263191699982, 577.7557437419891, 625.4425203800201, 673.4197454452515, 721.7970068454742, 770.0120685100555, 817.593169927597, 865.8052542209625, 914.1881167888641, 961.6973478794098, 1009.8715445995331, 1058.2066810131073, 1106.6703667640686, 1154.9812505245209, 1203.3916716575623, 1252.2294492721558, 1300.8279514312744, 1349.061047077179, 1397.8793287277222, 1446.5091571807861, 1494.8867506980896, 1543.32200050354, 1591.7262575626373, 1640.2007060050964, 1688.9551627635956, 1737.65087890625, 1786.030748128891, 1834.4513940811157, 1883.0217626094818, 1931.1752314567566, 1979.5841581821442, 2027.7253432273865, 2075.712546348572, 2123.131854057312, 2170.1486008167267, 2216.995007753372, 2262.5744421482086, 2308.326793193817, 2352.2553803920746]
