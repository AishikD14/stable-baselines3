No devices were found
Setting seed -  1
Creating multiple envs -  4
---------------------------------
Environment created
Box(-2.0, 2.0, (1,), float32) Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
Starting evaluation
0
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1537.9271037140054
avg cum rews: -1537.9271037140054, std: 0.0
the best agent: 0, best agent cum rewards: -1537.9271037140054
2
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -826.1290415264089
avg cum rews: -826.1290415264089, std: 0.0
the best agent: 0, best agent cum rewards: -826.1290415264089
4
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -368.3950238439725
avg cum rews: -368.3950238439725, std: 0.0
the best agent: 0, best agent cum rewards: -368.3950238439725
6
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -363.9464503165738
avg cum rews: -363.9464503165738, std: 0.0
the best agent: 0, best agent cum rewards: -363.9464503165738
8
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -371.80405363898427
avg cum rews: -371.80405363898427, std: 0.0
the best agent: 0, best agent cum rewards: -371.80405363898427
10
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -366.49237351171274
avg cum rews: -366.49237351171274, std: 0.0
the best agent: 0, best agent cum rewards: -366.49237351171274
12
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -365.34054559094534
avg cum rews: -365.34054559094534, std: 0.0
the best agent: 0, best agent cum rewards: -365.34054559094534
14
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -364.5603197615954
avg cum rews: -364.5603197615954, std: 0.0
the best agent: 0, best agent cum rewards: -364.5603197615954
16
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -363.85965277144606
avg cum rews: -363.85965277144606, std: 0.0
the best agent: 0, best agent cum rewards: -363.85965277144606
18
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -364.37843656478725
avg cum rews: -364.37843656478725, std: 0.0
the best agent: 0, best agent cum rewards: -364.37843656478725
20
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -497.71122853282696
avg cum rews: -497.71122853282696, std: 0.0
the best agent: 0, best agent cum rewards: -497.71122853282696
22
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -366.48963710282374
avg cum rews: -366.48963710282374, std: 0.0
the best agent: 0, best agent cum rewards: -366.48963710282374
24
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.63021714606754
avg cum rews: -360.63021714606754, std: 0.0
the best agent: 0, best agent cum rewards: -360.63021714606754
26
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -354.44007043396545
avg cum rews: -354.44007043396545, std: 0.0
the best agent: 0, best agent cum rewards: -354.44007043396545
28
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -488.7847015708593
avg cum rews: -488.7847015708593, std: 0.0
the best agent: 0, best agent cum rewards: -488.7847015708593
30
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -496.55991623041507
avg cum rews: -496.55991623041507, std: 0.0
the best agent: 0, best agent cum rewards: -496.55991623041507
32
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -496.59666418853357
avg cum rews: -496.59666418853357, std: 0.0
the best agent: 0, best agent cum rewards: -496.59666418853357
34
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -495.3556829922263
avg cum rews: -495.3556829922263, std: 0.0
the best agent: 0, best agent cum rewards: -495.3556829922263
36
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -496.5852030127825
avg cum rews: -496.5852030127825, std: 0.0
the best agent: 0, best agent cum rewards: -496.5852030127825
38
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -500.20475734339146
avg cum rews: -500.20475734339146, std: 0.0
the best agent: 0, best agent cum rewards: -500.20475734339146
40
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -498.8369852472305
avg cum rews: -498.8369852472305, std: 0.0
the best agent: 0, best agent cum rewards: -498.8369852472305
42
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -364.57780721761054
avg cum rews: -364.57780721761054, std: 0.0
the best agent: 0, best agent cum rewards: -364.57780721761054
44
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -362.6899222280957
avg cum rews: -362.6899222280957, std: 0.0
the best agent: 0, best agent cum rewards: -362.6899222280957
46
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.26759759281686
avg cum rews: -360.26759759281686, std: 0.0
the best agent: 0, best agent cum rewards: -360.26759759281686
Average distance of random agents to nearest neighbors: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Time taken for each iteration: [18.080707550048828, 36.53327298164368, 54.996840476989746, 74.01941323280334, 92.98471856117249, 112.90040969848633, 132.78358507156372, 152.48068618774414, 172.00931763648987, 191.80107712745667, 211.8316879272461, 231.63299894332886, 251.19154930114746, 270.9636278152466, 290.6952860355377, 309.9081699848175, 329.5086705684662, 349.17313623428345, 368.729927778244, 388.203186750412, 407.76241278648376, 426.7300798892975, 445.60266399383545, 464.5278537273407]
