No devices were found
Setting seed -  1
Creating multiple envs -  4
---------------------------------
Environment created
Box(-2.0, 2.0, (1,), float32) Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
Starting evaluation
0
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1564.2405852117852
avg cum rews: -1564.2405852117852, std: 0.0
the best agent: 0, best agent cum rewards: -1564.2405852117852
1
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1310.8434128017598
avg cum rews: -1310.8434128017598, std: 0.0
the best agent: 0, best agent cum rewards: -1310.8434128017598
2
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1043.2131010636147
avg cum rews: -1043.2131010636147, std: 0.0
the best agent: 0, best agent cum rewards: -1043.2131010636147
3
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -454.5734459440411
avg cum rews: -454.5734459440411, std: 0.0
the best agent: 0, best agent cum rewards: -454.5734459440411
4
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -448.11872231463417
avg cum rews: -448.11872231463417, std: 0.0
the best agent: 0, best agent cum rewards: -448.11872231463417
5
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -371.6071801315189
avg cum rews: -371.6071801315189, std: 0.0
the best agent: 0, best agent cum rewards: -371.6071801315189
6
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -361.6866745712432
avg cum rews: -361.6866745712432, std: 0.0
the best agent: 0, best agent cum rewards: -361.6866745712432
7
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.00627048793524
avg cum rews: -360.00627048793524, std: 0.0
the best agent: 0, best agent cum rewards: -360.00627048793524
8
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -361.95136567181356
avg cum rews: -361.95136567181356, std: 0.0
the best agent: 0, best agent cum rewards: -361.95136567181356
9
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -361.4470930655417
avg cum rews: -361.4470930655417, std: 0.0
the best agent: 0, best agent cum rewards: -361.4470930655417
10
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -362.11801638137075
avg cum rews: -362.11801638137075, std: 0.0
the best agent: 0, best agent cum rewards: -362.11801638137075
11
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.1514418955382
avg cum rews: -360.1514418955382, std: 0.0
the best agent: 0, best agent cum rewards: -360.1514418955382
12
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.5189292273454
avg cum rews: -360.5189292273454, std: 0.0
the best agent: 0, best agent cum rewards: -360.5189292273454
13
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.34007849934824
avg cum rews: -360.34007849934824, std: 0.0
the best agent: 0, best agent cum rewards: -360.34007849934824
14
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -363.2435419356405
avg cum rews: -363.2435419356405, std: 0.0
the best agent: 0, best agent cum rewards: -363.2435419356405
15
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -363.1715755179211
avg cum rews: -363.1715755179211, std: 0.0
the best agent: 0, best agent cum rewards: -363.1715755179211
16
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.9032760386301
avg cum rews: -360.9032760386301, std: 0.0
the best agent: 0, best agent cum rewards: -360.9032760386301
17
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -362.65325589085853
avg cum rews: -362.65325589085853, std: 0.0
the best agent: 0, best agent cum rewards: -362.65325589085853
18
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -359.275390434415
avg cum rews: -359.275390434415, std: 0.0
the best agent: 0, best agent cum rewards: -359.275390434415
19
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -359.1198205550634
avg cum rews: -359.1198205550634, std: 0.0
the best agent: 0, best agent cum rewards: -359.1198205550634
20
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -359.1728856233347
avg cum rews: -359.1728856233347, std: 0.0
the best agent: 0, best agent cum rewards: -359.1728856233347
21
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -358.79363240420446
avg cum rews: -358.79363240420446, std: 0.0
the best agent: 0, best agent cum rewards: -358.79363240420446
22
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -361.1140325610694
avg cum rews: -361.1140325610694, std: 0.0
the best agent: 0, best agent cum rewards: -361.1140325610694
23
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -361.5919042091805
avg cum rews: -361.5919042091805, std: 0.0
the best agent: 0, best agent cum rewards: -361.5919042091805
24
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -359.368017157851
avg cum rews: -359.368017157851, std: 0.0
the best agent: 0, best agent cum rewards: -359.368017157851
25
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -355.36880059064066
avg cum rews: -355.36880059064066, std: 0.0
the best agent: 0, best agent cum rewards: -355.36880059064066
26
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -353.63607888200386
avg cum rews: -353.63607888200386, std: 0.0
the best agent: 0, best agent cum rewards: -353.63607888200386
27
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -353.3660648665136
avg cum rews: -353.3660648665136, std: 0.0
the best agent: 0, best agent cum rewards: -353.3660648665136
28
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -351.6389662932802
avg cum rews: -351.6389662932802, std: 0.0
the best agent: 0, best agent cum rewards: -351.6389662932802
29
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -351.82009695534555
avg cum rews: -351.82009695534555, std: 0.0
the best agent: 0, best agent cum rewards: -351.82009695534555
30
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -352.06843947384124
avg cum rews: -352.06843947384124, std: 0.0
the best agent: 0, best agent cum rewards: -352.06843947384124
31
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -487.45439105562144
avg cum rews: -487.45439105562144, std: 0.0
the best agent: 0, best agent cum rewards: -487.45439105562144
32
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -357.1944741014539
avg cum rews: -357.1944741014539, std: 0.0
the best agent: 0, best agent cum rewards: -357.1944741014539
33
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.02061234399076
avg cum rews: -360.02061234399076, std: 0.0
the best agent: 0, best agent cum rewards: -360.02061234399076
34
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.17080140697425
avg cum rews: -360.17080140697425, std: 0.0
the best agent: 0, best agent cum rewards: -360.17080140697425
35
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -358.9913251323371
avg cum rews: -358.9913251323371, std: 0.0
the best agent: 0, best agent cum rewards: -358.9913251323371
36
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -360.5012198430547
avg cum rews: -360.5012198430547, std: 0.0
the best agent: 0, best agent cum rewards: -360.5012198430547
37
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -359.8406813752231
avg cum rews: -359.8406813752231, std: 0.0
the best agent: 0, best agent cum rewards: -359.8406813752231
38
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -357.1844687477913
avg cum rews: -357.1844687477913, std: 0.0
the best agent: 0, best agent cum rewards: -357.1844687477913
39
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -359.3177896513522
avg cum rews: -359.3177896513522, std: 0.0
the best agent: 0, best agent cum rewards: -359.3177896513522
40
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -359.7317798047861
avg cum rews: -359.7317798047861, std: 0.0
the best agent: 0, best agent cum rewards: -359.7317798047861
41
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -356.794916134335
avg cum rews: -356.794916134335, std: 0.0
the best agent: 0, best agent cum rewards: -356.794916134335
42
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -356.72233582937
avg cum rews: -356.72233582937, std: 0.0
the best agent: 0, best agent cum rewards: -356.72233582937
43
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -357.73642074571114
avg cum rews: -357.73642074571114, std: 0.0
the best agent: 0, best agent cum rewards: -357.73642074571114
44
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -359.16151804410595
avg cum rews: -359.16151804410595, std: 0.0
the best agent: 0, best agent cum rewards: -359.16151804410595
45
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -358.7982105917768
avg cum rews: -358.7982105917768, std: 0.0
the best agent: 0, best agent cum rewards: -358.7982105917768
46
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -358.6951266281968
avg cum rews: -358.6951266281968, std: 0.0
the best agent: 0, best agent cum rewards: -358.6951266281968
47
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -357.7127917247555
avg cum rews: -357.7127917247555, std: 0.0
the best agent: 0, best agent cum rewards: -357.7127917247555
Average distance of random agents to nearest neighbors: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Time taken for each iteration: [7.815263986587524, 15.663516283035278, 23.491291999816895, 31.329737424850464, 39.164246559143066, 47.00113391876221, 54.855780839920044, 62.702359676361084, 70.56564450263977, 78.4163646697998, 86.29695677757263, 94.16374015808105, 102.05277371406555, 109.92623782157898, 117.82261347770691, 125.69140243530273, 133.55161905288696, 141.43321442604065, 149.27816438674927, 157.11254382133484, 164.98256659507751, 172.86086797714233, 180.73995733261108, 188.63343334197998, 196.50742101669312, 204.42600512504578, 212.31403064727783, 220.17809796333313, 228.05205655097961, 235.91209077835083, 243.77580451965332, 251.63984394073486, 259.5022654533386, 267.3630907535553, 275.2111804485321, 283.0849223136902, 290.9719388484955, 298.8452219963074, 306.7022166252136, 314.5681691169739, 322.43973565101624, 330.32473945617676, 338.1880524158478, 346.0672221183777, 353.93115043640137, 361.79666900634766, 369.64943385124207, 377.5130777359009]
