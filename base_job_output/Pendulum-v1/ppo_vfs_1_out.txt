No devices were found
Setting seed -  0
Creating multiple envs -  4
---------------------------------
Environment created
Box(-2.0, 2.0, (1,), float32) Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
Starting evaluation
0
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1572.5328965793922
avg cum rews: -1572.5328965793922, std: 0.0
the best agent: 0, best agent cum rewards: -1572.5328965793922
1
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1397.3733135891478
avg cum rews: -1397.3733135891478, std: 0.0
the best agent: 0, best agent cum rewards: -1397.3733135891478
2
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -914.0395708109193
avg cum rews: -914.0395708109193, std: 0.0
the best agent: 0, best agent cum rewards: -914.0395708109193
3
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -133.56469171650582
avg cum rews: -133.56469171650582, std: 0.0
the best agent: 0, best agent cum rewards: -133.56469171650582
4
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -135.58768180217402
avg cum rews: -135.58768180217402, std: 0.0
the best agent: 0, best agent cum rewards: -135.58768180217402
5
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.29350324040274
avg cum rews: -131.29350324040274, std: 0.0
the best agent: 0, best agent cum rewards: -131.29350324040274
6
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.2294732445844
avg cum rews: -127.2294732445844, std: 0.0
the best agent: 0, best agent cum rewards: -127.2294732445844
7
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.44492458538484
avg cum rews: -127.44492458538484, std: 0.0
the best agent: 0, best agent cum rewards: -127.44492458538484
8
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.30737379721022
avg cum rews: -127.30737379721022, std: 0.0
the best agent: 0, best agent cum rewards: -127.30737379721022
9
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.17344081577022
avg cum rews: -127.17344081577022, std: 0.0
the best agent: 0, best agent cum rewards: -127.17344081577022
10
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.0731294779165
avg cum rews: -127.0731294779165, std: 0.0
the best agent: 0, best agent cum rewards: -127.0731294779165
11
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -126.83703912531479
avg cum rews: -126.83703912531479, std: 0.0
the best agent: 0, best agent cum rewards: -126.83703912531479
12
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.72808476696916
avg cum rews: -127.72808476696916, std: 0.0
the best agent: 0, best agent cum rewards: -127.72808476696916
13
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -126.7235457171385
avg cum rews: -126.7235457171385, std: 0.0
the best agent: 0, best agent cum rewards: -126.7235457171385
14
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.21407098203257
avg cum rews: -127.21407098203257, std: 0.0
the best agent: 0, best agent cum rewards: -127.21407098203257
15
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.61567639497677
avg cum rews: -127.61567639497677, std: 0.0
the best agent: 0, best agent cum rewards: -127.61567639497677
16
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.06638299861851
avg cum rews: -127.06638299861851, std: 0.0
the best agent: 0, best agent cum rewards: -127.06638299861851
17
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.6373668514368
avg cum rews: -127.6373668514368, std: 0.0
the best agent: 0, best agent cum rewards: -127.6373668514368
18
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.07420282000103
avg cum rews: -129.07420282000103, std: 0.0
the best agent: 0, best agent cum rewards: -129.07420282000103
19
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -128.75739173532364
avg cum rews: -128.75739173532364, std: 0.0
the best agent: 0, best agent cum rewards: -128.75739173532364
20
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.3101481026292
avg cum rews: -129.3101481026292, std: 0.0
the best agent: 0, best agent cum rewards: -129.3101481026292
21
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.42454126986863
avg cum rews: -129.42454126986863, std: 0.0
the best agent: 0, best agent cum rewards: -129.42454126986863
22
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.52605678235759
avg cum rews: -130.52605678235759, std: 0.0
the best agent: 0, best agent cum rewards: -130.52605678235759
23
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.46006308239484
avg cum rews: -130.46006308239484, std: 0.0
the best agent: 0, best agent cum rewards: -130.46006308239484
24
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.16746795496164
avg cum rews: -130.16746795496164, std: 0.0
the best agent: 0, best agent cum rewards: -130.16746795496164
25
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.2481360970727
avg cum rews: -129.2481360970727, std: 0.0
the best agent: 0, best agent cum rewards: -129.2481360970727
26
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -133.02046318627222
avg cum rews: -133.02046318627222, std: 0.0
the best agent: 0, best agent cum rewards: -133.02046318627222
27
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -266.39498854180414
avg cum rews: -266.39498854180414, std: 0.0
the best agent: 0, best agent cum rewards: -266.39498854180414
28
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -128.06018475056212
avg cum rews: -128.06018475056212, std: 0.0
the best agent: 0, best agent cum rewards: -128.06018475056212
29
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -126.56939372479683
avg cum rews: -126.56939372479683, std: 0.0
the best agent: 0, best agent cum rewards: -126.56939372479683
30
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.30051164403729
avg cum rews: -127.30051164403729, std: 0.0
the best agent: 0, best agent cum rewards: -127.30051164403729
31
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.21363093821466
avg cum rews: -127.21363093821466, std: 0.0
the best agent: 0, best agent cum rewards: -127.21363093821466
32
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.72582943009071
avg cum rews: -127.72582943009071, std: 0.0
the best agent: 0, best agent cum rewards: -127.72582943009071
33
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.10251565770372
avg cum rews: -129.10251565770372, std: 0.0
the best agent: 0, best agent cum rewards: -129.10251565770372
34
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -128.88192492213832
avg cum rews: -128.88192492213832, std: 0.0
the best agent: 0, best agent cum rewards: -128.88192492213832
35
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.1881004243129
avg cum rews: -129.1881004243129, std: 0.0
the best agent: 0, best agent cum rewards: -129.1881004243129
36
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.0539919281903
avg cum rews: -130.0539919281903, std: 0.0
the best agent: 0, best agent cum rewards: -130.0539919281903
37
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.03112687460154
avg cum rews: -130.03112687460154, std: 0.0
the best agent: 0, best agent cum rewards: -130.03112687460154
38
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.86381406964244
avg cum rews: -129.86381406964244, std: 0.0
the best agent: 0, best agent cum rewards: -129.86381406964244
39
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.57112053358512
avg cum rews: -130.57112053358512, std: 0.0
the best agent: 0, best agent cum rewards: -130.57112053358512
40
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.32711714984094
avg cum rews: -129.32711714984094, std: 0.0
the best agent: 0, best agent cum rewards: -129.32711714984094
41
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.56777644343404
avg cum rews: -129.56777644343404, std: 0.0
the best agent: 0, best agent cum rewards: -129.56777644343404
42
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.23892939386516
avg cum rews: -129.23892939386516, std: 0.0
the best agent: 0, best agent cum rewards: -129.23892939386516
43
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.61617573944653
avg cum rews: -130.61617573944653, std: 0.0
the best agent: 0, best agent cum rewards: -130.61617573944653
44
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.12456084083564
avg cum rews: -130.12456084083564, std: 0.0
the best agent: 0, best agent cum rewards: -130.12456084083564
45
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.3892673075231
avg cum rews: -130.3892673075231, std: 0.0
the best agent: 0, best agent cum rewards: -130.3892673075231
46
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.9683192062603
avg cum rews: -129.9683192062603, std: 0.0
the best agent: 0, best agent cum rewards: -129.9683192062603
47
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -128.9009010516025
avg cum rews: -128.9009010516025, std: 0.0
the best agent: 0, best agent cum rewards: -128.9009010516025
Average distance of random agents to nearest neighbors: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Time taken for each iteration: [7.775696277618408, 15.537256002426147, 23.37724781036377, 31.217713117599487, 39.07813310623169, 46.972493171691895, 54.81342649459839, 62.64599609375, 70.46900177001953, 78.31543111801147, 86.13265776634216, 93.906574010849, 101.71069383621216, 109.59812450408936, 117.46633124351501, 125.30824375152588, 133.14194655418396, 140.98885846138, 148.87464833259583, 156.6890938282013, 164.45055031776428, 172.35235166549683, 180.23439073562622, 188.11151123046875, 195.9778928756714, 203.8639256954193, 211.7223460674286, 219.6092607975006, 227.5154275894165, 235.4216616153717, 243.3432013988495, 251.24861788749695, 259.1712443828583, 267.0619466304779, 274.9373185634613, 282.82157254219055, 290.7221825122833, 298.67305421829224, 306.534024477005, 314.412859916687, 322.297203540802, 330.18812918663025, 338.0628402233124, 345.93626594543457, 353.83317494392395, 361.7015714645386, 369.59949016571045, 377.4879639148712]
