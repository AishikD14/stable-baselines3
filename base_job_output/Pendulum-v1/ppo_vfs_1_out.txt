No devices were found
Setting seed -  0
Creating multiple envs -  4
---------------------------------
Environment created
Box(-2.0, 2.0, (1,), float32) Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
Starting evaluation
0
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1321.7089350744743
avg cum rews: -1321.7089350744743, std: 0.0
the best agent: 0, best agent cum rewards: -1321.7089350744743
2
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -412.84919858396455
avg cum rews: -412.84919858396455, std: 0.0
the best agent: 0, best agent cum rewards: -412.84919858396455
4
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.23836217802025
avg cum rews: -131.23836217802025, std: 0.0
the best agent: 0, best agent cum rewards: -131.23836217802025
6
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.33212082467398
avg cum rews: -129.33212082467398, std: 0.0
the best agent: 0, best agent cum rewards: -129.33212082467398
8
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.49686353735814
avg cum rews: -127.49686353735814, std: 0.0
the best agent: 0, best agent cum rewards: -127.49686353735814
10
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -124.96676909743489
avg cum rews: -124.96676909743489, std: 0.0
the best agent: 0, best agent cum rewards: -124.96676909743489
12
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -125.65503339006382
avg cum rews: -125.65503339006382, std: 0.0
the best agent: 0, best agent cum rewards: -125.65503339006382
14
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -124.35932401315658
avg cum rews: -124.35932401315658, std: 0.0
the best agent: 0, best agent cum rewards: -124.35932401315658
16
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -126.27990046914202
avg cum rews: -126.27990046914202, std: 0.0
the best agent: 0, best agent cum rewards: -126.27990046914202
18
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.58587089507407
avg cum rews: -127.58587089507407, std: 0.0
the best agent: 0, best agent cum rewards: -127.58587089507407
20
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.29989224819715
avg cum rews: -131.29989224819715, std: 0.0
the best agent: 0, best agent cum rewards: -131.29989224819715
22
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.15070694334358
avg cum rews: -130.15070694334358, std: 0.0
the best agent: 0, best agent cum rewards: -130.15070694334358
24
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.44080950613684
avg cum rews: -129.44080950613684, std: 0.0
the best agent: 0, best agent cum rewards: -129.44080950613684
26
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.30881503727608
avg cum rews: -130.30881503727608, std: 0.0
the best agent: 0, best agent cum rewards: -130.30881503727608
28
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -128.23108835186954
avg cum rews: -128.23108835186954, std: 0.0
the best agent: 0, best agent cum rewards: -128.23108835186954
30
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -128.89065161780263
avg cum rews: -128.89065161780263, std: 0.0
the best agent: 0, best agent cum rewards: -128.89065161780263
32
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.2386576351924
avg cum rews: -130.2386576351924, std: 0.0
the best agent: 0, best agent cum rewards: -130.2386576351924
34
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -126.85398974595785
avg cum rews: -126.85398974595785, std: 0.0
the best agent: 0, best agent cum rewards: -126.85398974595785
36
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.39608048820175
avg cum rews: -127.39608048820175, std: 0.0
the best agent: 0, best agent cum rewards: -127.39608048820175
38
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -127.01634869602755
avg cum rews: -127.01634869602755, std: 0.0
the best agent: 0, best agent cum rewards: -127.01634869602755
40
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.43571427371327
avg cum rews: -130.43571427371327, std: 0.0
the best agent: 0, best agent cum rewards: -130.43571427371327
42
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.74223835008425
avg cum rews: -130.74223835008425, std: 0.0
the best agent: 0, best agent cum rewards: -130.74223835008425
44
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -267.8369061505813
avg cum rews: -267.8369061505813, std: 0.0
the best agent: 0, best agent cum rewards: -267.8369061505813
46
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -266.485191888756
avg cum rews: -266.485191888756, std: 0.0
the best agent: 0, best agent cum rewards: -266.485191888756
Average distance of random agents to nearest neighbors: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Time taken for each iteration: [17.697157621383667, 35.629974603652954, 53.793787717819214, 71.92562866210938, 90.49834537506104, 108.97536659240723, 127.68801236152649, 146.6290955543518, 165.81266713142395, 185.24109840393066, 204.71776008605957, 224.15268349647522, 244.0979506969452, 263.5745053291321, 283.09957575798035, 302.75519943237305, 322.2074544429779, 341.8012099266052, 361.2236821651459, 380.83466625213623, 400.66417050361633, 420.29322957992554, 439.62728691101074, 459.0613088607788]
