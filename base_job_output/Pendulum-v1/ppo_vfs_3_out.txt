No devices were found
Setting seed -  2
Creating multiple envs -  4
---------------------------------
Environment created
Box(-2.0, 2.0, (1,), float32) Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
Starting evaluation
0
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -933.0794130484678
avg cum rews: -933.0794130484678, std: 0.0
the best agent: 0, best agent cum rewards: -933.0794130484678
2
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -609.1377030353482
avg cum rews: -609.1377030353482, std: 0.0
the best agent: 0, best agent cum rewards: -609.1377030353482
4
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.1240155124392999
avg cum rews: -1.1240155124392999, std: 0.0
the best agent: 0, best agent cum rewards: -1.1240155124392999
6
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.3080061738019204
avg cum rews: -0.3080061738019204, std: 0.0
the best agent: 0, best agent cum rewards: -0.3080061738019204
8
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.31943969863996374
avg cum rews: -0.31943969863996374, std: 0.0
the best agent: 0, best agent cum rewards: -0.31943969863996374
10
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.3947328551619581
avg cum rews: -0.3947328551619581, std: 0.0
the best agent: 0, best agent cum rewards: -0.3947328551619581
12
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.08993077118196788
avg cum rews: -0.08993077118196788, std: 0.0
the best agent: 0, best agent cum rewards: -0.08993077118196788
14
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.6841745808261014
avg cum rews: -0.6841745808261014, std: 0.0
the best agent: 0, best agent cum rewards: -0.6841745808261014
16
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.7567108208887475
avg cum rews: -0.7567108208887475, std: 0.0
the best agent: 0, best agent cum rewards: -0.7567108208887475
18
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.6503465723865925
avg cum rews: -0.6503465723865925, std: 0.0
the best agent: 0, best agent cum rewards: -0.6503465723865925
20
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.049204397572543
avg cum rews: -1.049204397572543, std: 0.0
the best agent: 0, best agent cum rewards: -1.049204397572543
22
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.41511391543459897
avg cum rews: -0.41511391543459897, std: 0.0
the best agent: 0, best agent cum rewards: -0.41511391543459897
24
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.22012506048337208
avg cum rews: -0.22012506048337208, std: 0.0
the best agent: 0, best agent cum rewards: -0.22012506048337208
26
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.08926865474534228
avg cum rews: -0.08926865474534228, std: 0.0
the best agent: 0, best agent cum rewards: -0.08926865474534228
28
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.09362723726865545
avg cum rews: -0.09362723726865545, std: 0.0
the best agent: 0, best agent cum rewards: -0.09362723726865545
30
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.2704202262775997
avg cum rews: -0.2704202262775997, std: 0.0
the best agent: 0, best agent cum rewards: -0.2704202262775997
32
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.1428010985524606
avg cum rews: -0.1428010985524606, std: 0.0
the best agent: 0, best agent cum rewards: -0.1428010985524606
34
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.08977571221505597
avg cum rews: -0.08977571221505597, std: 0.0
the best agent: 0, best agent cum rewards: -0.08977571221505597
36
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.09436224689903443
avg cum rews: -0.09436224689903443, std: 0.0
the best agent: 0, best agent cum rewards: -0.09436224689903443
38
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.09973576502603704
avg cum rews: -0.09973576502603704, std: 0.0
the best agent: 0, best agent cum rewards: -0.09973576502603704
40
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.09599495025390109
avg cum rews: -0.09599495025390109, std: 0.0
the best agent: 0, best agent cum rewards: -0.09599495025390109
42
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.10514719115605684
avg cum rews: -0.10514719115605684, std: 0.0
the best agent: 0, best agent cum rewards: -0.10514719115605684
44
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.19639676460789493
avg cum rews: -0.19639676460789493, std: 0.0
the best agent: 0, best agent cum rewards: -0.19639676460789493
46
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.09502625212095646
avg cum rews: -0.09502625212095646, std: 0.0
the best agent: 0, best agent cum rewards: -0.09502625212095646
Average distance of random agents to nearest neighbors: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Time taken for each iteration: [18.4348623752594, 36.866828203201294, 55.38435459136963, 74.74354863166809, 94.47983169555664, 113.59252786636353, 132.85431122779846, 152.26713466644287, 171.78949785232544, 191.4271743297577, 210.86550092697144, 230.5017650127411, 249.7875633239746, 268.8885807991028, 289.00958251953125, 308.73638796806335, 328.22571420669556, 347.3121168613434, 366.5573856830597, 385.52304768562317, 404.13770055770874, 422.6716957092285, 440.8601801395416, 458.89024090766907]
