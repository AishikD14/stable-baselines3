No devices were found
Setting seed -  2
Creating multiple envs -  4
---------------------------------
Environment created
Box(-2.0, 2.0, (1,), float32) Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
Starting evaluation
0
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1099.46818419102
avg cum rews: -1099.46818419102, std: 0.0
the best agent: 0, best agent cum rewards: -1099.46818419102
1
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -863.3688629344153
avg cum rews: -863.3688629344153, std: 0.0
the best agent: 0, best agent cum rewards: -863.3688629344153
2
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -678.6803783062439
avg cum rews: -678.6803783062439, std: 0.0
the best agent: 0, best agent cum rewards: -678.6803783062439
3
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -541.5108050100945
avg cum rews: -541.5108050100945, std: 0.0
the best agent: 0, best agent cum rewards: -541.5108050100945
4
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -175.36421776138909
avg cum rews: -175.36421776138909, std: 0.0
the best agent: 0, best agent cum rewards: -175.36421776138909
5
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -2.8193060162550085
avg cum rews: -2.8193060162550085, std: 0.0
the best agent: 0, best agent cum rewards: -2.8193060162550085
6
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.5413757702037978
avg cum rews: -0.5413757702037978, std: 0.0
the best agent: 0, best agent cum rewards: -0.5413757702037978
7
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.09385642174629899
avg cum rews: -0.09385642174629899, std: 0.0
the best agent: 0, best agent cum rewards: -0.09385642174629899
8
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.347287258670675
avg cum rews: -1.347287258670675, std: 0.0
the best agent: 0, best agent cum rewards: -1.347287258670675
9
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.8092764162606916
avg cum rews: -1.8092764162606916, std: 0.0
the best agent: 0, best agent cum rewards: -1.8092764162606916
10
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.8460310839262821
avg cum rews: -1.8460310839262821, std: 0.0
the best agent: 0, best agent cum rewards: -1.8460310839262821
11
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.6582902476226761
avg cum rews: -0.6582902476226761, std: 0.0
the best agent: 0, best agent cum rewards: -0.6582902476226761
12
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.8519866843300097
avg cum rews: -1.8519866843300097, std: 0.0
the best agent: 0, best agent cum rewards: -1.8519866843300097
13
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.1580837808731468
avg cum rews: -1.1580837808731468, std: 0.0
the best agent: 0, best agent cum rewards: -1.1580837808731468
14
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.9907078030580116
avg cum rews: -1.9907078030580116, std: 0.0
the best agent: 0, best agent cum rewards: -1.9907078030580116
15
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.2122928317870816
avg cum rews: -1.2122928317870816, std: 0.0
the best agent: 0, best agent cum rewards: -1.2122928317870816
16
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.1505352944426295
avg cum rews: -1.1505352944426295, std: 0.0
the best agent: 0, best agent cum rewards: -1.1505352944426295
17
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.2169047852813226
avg cum rews: -1.2169047852813226, std: 0.0
the best agent: 0, best agent cum rewards: -1.2169047852813226
18
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.40308330313303886
avg cum rews: -0.40308330313303886, std: 0.0
the best agent: 0, best agent cum rewards: -0.40308330313303886
19
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.5788263143865999
avg cum rews: -0.5788263143865999, std: 0.0
the best agent: 0, best agent cum rewards: -0.5788263143865999
20
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.3169955265898791
avg cum rews: -0.3169955265898791, std: 0.0
the best agent: 0, best agent cum rewards: -0.3169955265898791
21
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.10893644564863125
avg cum rews: -0.10893644564863125, std: 0.0
the best agent: 0, best agent cum rewards: -0.10893644564863125
22
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.09142943043170663
avg cum rews: -0.09142943043170663, std: 0.0
the best agent: 0, best agent cum rewards: -0.09142943043170663
23
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.09795872984777933
avg cum rews: -0.09795872984777933, std: 0.0
the best agent: 0, best agent cum rewards: -0.09795872984777933
24
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.09247382661229887
avg cum rews: -0.09247382661229887, std: 0.0
the best agent: 0, best agent cum rewards: -0.09247382661229887
25
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.2168943206266949
avg cum rews: -0.2168943206266949, std: 0.0
the best agent: 0, best agent cum rewards: -0.2168943206266949
26
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.2720569362866752
avg cum rews: -0.2720569362866752, std: 0.0
the best agent: 0, best agent cum rewards: -0.2720569362866752
27
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.32491416858902844
avg cum rews: -0.32491416858902844, std: 0.0
the best agent: 0, best agent cum rewards: -0.32491416858902844
28
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.3507622399586017
avg cum rews: -0.3507622399586017, std: 0.0
the best agent: 0, best agent cum rewards: -0.3507622399586017
29
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.4365858139027832
avg cum rews: -0.4365858139027832, std: 0.0
the best agent: 0, best agent cum rewards: -0.4365858139027832
30
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.26485597557399687
avg cum rews: -0.26485597557399687, std: 0.0
the best agent: 0, best agent cum rewards: -0.26485597557399687
31
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.2526335278110876
avg cum rews: -0.2526335278110876, std: 0.0
the best agent: 0, best agent cum rewards: -0.2526335278110876
32
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.7183677273767987
avg cum rews: -0.7183677273767987, std: 0.0
the best agent: 0, best agent cum rewards: -0.7183677273767987
33
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.0419976253750554
avg cum rews: -1.0419976253750554, std: 0.0
the best agent: 0, best agent cum rewards: -1.0419976253750554
34
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -2.090856266620255
avg cum rews: -2.090856266620255, std: 0.0
the best agent: 0, best agent cum rewards: -2.090856266620255
35
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.9891301532719732
avg cum rews: -1.9891301532719732, std: 0.0
the best agent: 0, best agent cum rewards: -1.9891301532719732
36
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -2.01512579693698
avg cum rews: -2.01512579693698, std: 0.0
the best agent: 0, best agent cum rewards: -2.01512579693698
37
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.8623661597918653
avg cum rews: -1.8623661597918653, std: 0.0
the best agent: 0, best agent cum rewards: -1.8623661597918653
38
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.9056930781039776
avg cum rews: -1.9056930781039776, std: 0.0
the best agent: 0, best agent cum rewards: -1.9056930781039776
39
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.5577493134068359
avg cum rews: -1.5577493134068359, std: 0.0
the best agent: 0, best agent cum rewards: -1.5577493134068359
40
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.242578259999757
avg cum rews: -1.242578259999757, std: 0.0
the best agent: 0, best agent cum rewards: -1.242578259999757
41
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.4134478139717732
avg cum rews: -1.4134478139717732, std: 0.0
the best agent: 0, best agent cum rewards: -1.4134478139717732
42
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.457064209468325
avg cum rews: -1.457064209468325, std: 0.0
the best agent: 0, best agent cum rewards: -1.457064209468325
43
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.6226311707057959
avg cum rews: -1.6226311707057959, std: 0.0
the best agent: 0, best agent cum rewards: -1.6226311707057959
44
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1.0684265537640125
avg cum rews: -1.0684265537640125, std: 0.0
the best agent: 0, best agent cum rewards: -1.0684265537640125
45
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.9559007619645853
avg cum rews: -0.9559007619645853, std: 0.0
the best agent: 0, best agent cum rewards: -0.9559007619645853
46
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.4614248693953171
avg cum rews: -0.4614248693953171, std: 0.0
the best agent: 0, best agent cum rewards: -0.4614248693953171
47
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -0.979386356194053
avg cum rews: -0.979386356194053, std: 0.0
the best agent: 0, best agent cum rewards: -0.979386356194053
Average distance of random agents to nearest neighbors: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Time taken for each iteration: [7.840405464172363, 15.709049701690674, 23.59826683998108, 31.447121620178223, 39.32937741279602, 47.17282319068909, 55.054805755615234, 62.91922044754028, 70.78835439682007, 78.65667176246643, 86.50975561141968, 94.37108087539673, 102.2498505115509, 110.11815214157104, 117.9883484840393, 125.85179162025452, 133.70203042030334, 141.5645740032196, 149.4216113090515, 157.2878429889679, 165.13218188285828, 173.05939626693726, 181.0347502231598, 188.85398292541504, 196.66903591156006, 204.48954153060913, 212.29585814476013, 220.11087441444397, 227.9255211353302, 235.74087500572205, 243.57860589027405, 251.3936665058136, 259.20889377593994, 267.02264404296875, 274.8338656425476, 282.6494793891907, 290.46783804893494, 298.2914717197418, 306.09628462791443, 313.91077613830566, 321.7087576389313, 329.52489161491394, 337.32932353019714, 345.1411669254303, 352.95163440704346, 360.7786076068878, 368.58999252319336, 376.40547919273376]
