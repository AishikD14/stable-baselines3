No devices were found
Setting seed -  3
Creating multiple envs -  4
---------------------------------
Environment created
Box(-2.0, 2.0, (1,), float32) Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
Starting evaluation
0
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -995.5162029064338
avg cum rews: -995.5162029064338, std: 0.0
the best agent: 0, best agent cum rewards: -995.5162029064338
2
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -465.51770701348255
avg cum rews: -465.51770701348255, std: 0.0
the best agent: 0, best agent cum rewards: -465.51770701348255
4
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.58673024103143
avg cum rews: -131.58673024103143, std: 0.0
the best agent: 0, best agent cum rewards: -131.58673024103143
6
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.4881482906246
avg cum rews: -130.4881482906246, std: 0.0
the best agent: 0, best agent cum rewards: -130.4881482906246
8
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.76208182099916
avg cum rews: -132.76208182099916, std: 0.0
the best agent: 0, best agent cum rewards: -132.76208182099916
10
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.13436610696004
avg cum rews: -130.13436610696004, std: 0.0
the best agent: 0, best agent cum rewards: -130.13436610696004
12
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.08728646790746
avg cum rews: -132.08728646790746, std: 0.0
the best agent: 0, best agent cum rewards: -132.08728646790746
14
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -134.54564612449656
avg cum rews: -134.54564612449656, std: 0.0
the best agent: 0, best agent cum rewards: -134.54564612449656
16
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.97917064077677
avg cum rews: -132.97917064077677, std: 0.0
the best agent: 0, best agent cum rewards: -132.97917064077677
18
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.80784183497576
avg cum rews: -132.80784183497576, std: 0.0
the best agent: 0, best agent cum rewards: -132.80784183497576
20
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.83603950801145
avg cum rews: -131.83603950801145, std: 0.0
the best agent: 0, best agent cum rewards: -131.83603950801145
22
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.0539003392398
avg cum rews: -131.0539003392398, std: 0.0
the best agent: 0, best agent cum rewards: -131.0539003392398
24
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.15982859746438
avg cum rews: -131.15982859746438, std: 0.0
the best agent: 0, best agent cum rewards: -131.15982859746438
26
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.62053124362106
avg cum rews: -131.62053124362106, std: 0.0
the best agent: 0, best agent cum rewards: -131.62053124362106
28
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.73992960859536
avg cum rews: -132.73992960859536, std: 0.0
the best agent: 0, best agent cum rewards: -132.73992960859536
30
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.83088462984645
avg cum rews: -132.83088462984645, std: 0.0
the best agent: 0, best agent cum rewards: -132.83088462984645
32
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.04511957602102
avg cum rews: -132.04511957602102, std: 0.0
the best agent: 0, best agent cum rewards: -132.04511957602102
34
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.53247138512958
avg cum rews: -131.53247138512958, std: 0.0
the best agent: 0, best agent cum rewards: -131.53247138512958
36
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.97105564999916
avg cum rews: -129.97105564999916, std: 0.0
the best agent: 0, best agent cum rewards: -129.97105564999916
38
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.81335424187753
avg cum rews: -130.81335424187753, std: 0.0
the best agent: 0, best agent cum rewards: -130.81335424187753
40
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.7492717257721
avg cum rews: -130.7492717257721, std: 0.0
the best agent: 0, best agent cum rewards: -130.7492717257721
42
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.6068944040918
avg cum rews: -131.6068944040918, std: 0.0
the best agent: 0, best agent cum rewards: -131.6068944040918
44
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.4026247020956
avg cum rews: -130.4026247020956, std: 0.0
the best agent: 0, best agent cum rewards: -130.4026247020956
46
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.83785050821476
avg cum rews: -130.83785050821476, std: 0.0
the best agent: 0, best agent cum rewards: -130.83785050821476
Average distance of random agents to nearest neighbors: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Time taken for each iteration: [19.47536325454712, 38.90330743789673, 58.266135931015015, 77.68760967254639, 97.2042760848999, 116.71417045593262, 136.15822291374207, 156.12664270401, 175.44889545440674, 194.8950915336609, 214.3676679134369, 233.9095664024353, 253.2372908592224, 272.5281488895416, 291.9611234664917, 311.1635525226593, 329.8914351463318, 348.7244122028351, 367.18311405181885, 385.2348473072052, 402.8952581882477, 420.4842920303345, 438.0236175060272, 455.5110867023468]
