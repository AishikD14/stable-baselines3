No devices were found
Setting seed -  3
Creating multiple envs -  4
---------------------------------
Environment created
Box(-2.0, 2.0, (1,), float32) Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)
Starting evaluation
0
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1412.4817721363627
avg cum rews: -1412.4817721363627, std: 0.0
the best agent: 0, best agent cum rewards: -1412.4817721363627
1
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -1171.5833471341664
avg cum rews: -1171.5833471341664, std: 0.0
the best agent: 0, best agent cum rewards: -1171.5833471341664
2
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -921.2039864150057
avg cum rews: -921.2039864150057, std: 0.0
the best agent: 0, best agent cum rewards: -921.2039864150057
3
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -525.6737167379879
avg cum rews: -525.6737167379879, std: 0.0
the best agent: 0, best agent cum rewards: -525.6737167379879
4
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -139.57297115384205
avg cum rews: -139.57297115384205, std: 0.0
the best agent: 0, best agent cum rewards: -139.57297115384205
5
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.95917030308678
avg cum rews: -130.95917030308678, std: 0.0
the best agent: 0, best agent cum rewards: -130.95917030308678
6
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.03654214320647
avg cum rews: -129.03654214320647, std: 0.0
the best agent: 0, best agent cum rewards: -129.03654214320647
7
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -128.85783741914523
avg cum rews: -128.85783741914523, std: 0.0
the best agent: 0, best agent cum rewards: -128.85783741914523
8
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.6038393131589
avg cum rews: -129.6038393131589, std: 0.0
the best agent: 0, best agent cum rewards: -129.6038393131589
9
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.40367432307994
avg cum rews: -129.40367432307994, std: 0.0
the best agent: 0, best agent cum rewards: -129.40367432307994
10
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -129.5786548265509
avg cum rews: -129.5786548265509, std: 0.0
the best agent: 0, best agent cum rewards: -129.5786548265509
11
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.57558592114498
avg cum rews: -130.57558592114498, std: 0.0
the best agent: 0, best agent cum rewards: -130.57558592114498
12
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.37124290518398
avg cum rews: -131.37124290518398, std: 0.0
the best agent: 0, best agent cum rewards: -131.37124290518398
13
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.6273554974278
avg cum rews: -131.6273554974278, std: 0.0
the best agent: 0, best agent cum rewards: -131.6273554974278
14
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.17235712714412
avg cum rews: -130.17235712714412, std: 0.0
the best agent: 0, best agent cum rewards: -130.17235712714412
15
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.69729272335027
avg cum rews: -130.69729272335027, std: 0.0
the best agent: 0, best agent cum rewards: -130.69729272335027
16
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.71935313616584
avg cum rews: -132.71935313616584, std: 0.0
the best agent: 0, best agent cum rewards: -132.71935313616584
17
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -133.35110431955277
avg cum rews: -133.35110431955277, std: 0.0
the best agent: 0, best agent cum rewards: -133.35110431955277
18
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.50501350897164
avg cum rews: -131.50501350897164, std: 0.0
the best agent: 0, best agent cum rewards: -131.50501350897164
19
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.85865365910382
avg cum rews: -132.85865365910382, std: 0.0
the best agent: 0, best agent cum rewards: -132.85865365910382
20
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.32075824763584
avg cum rews: -132.32075824763584, std: 0.0
the best agent: 0, best agent cum rewards: -132.32075824763584
21
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.1952400010646
avg cum rews: -132.1952400010646, std: 0.0
the best agent: 0, best agent cum rewards: -132.1952400010646
22
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.33626500426058
avg cum rews: -131.33626500426058, std: 0.0
the best agent: 0, best agent cum rewards: -131.33626500426058
23
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.71099224955196
avg cum rews: -132.71099224955196, std: 0.0
the best agent: 0, best agent cum rewards: -132.71099224955196
24
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.39369402512423
avg cum rews: -132.39369402512423, std: 0.0
the best agent: 0, best agent cum rewards: -132.39369402512423
25
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.07683229487571
avg cum rews: -132.07683229487571, std: 0.0
the best agent: 0, best agent cum rewards: -132.07683229487571
26
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.1439341505978
avg cum rews: -132.1439341505978, std: 0.0
the best agent: 0, best agent cum rewards: -132.1439341505978
27
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.46271864202157
avg cum rews: -131.46271864202157, std: 0.0
the best agent: 0, best agent cum rewards: -131.46271864202157
28
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.80646638808844
avg cum rews: -130.80646638808844, std: 0.0
the best agent: 0, best agent cum rewards: -130.80646638808844
29
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.2250760377101
avg cum rews: -130.2250760377101, std: 0.0
the best agent: 0, best agent cum rewards: -130.2250760377101
30
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.59814828628808
avg cum rews: -131.59814828628808, std: 0.0
the best agent: 0, best agent cum rewards: -131.59814828628808
31
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.12352313641873
avg cum rews: -132.12352313641873, std: 0.0
the best agent: 0, best agent cum rewards: -132.12352313641873
32
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.6638810247526
avg cum rews: -131.6638810247526, std: 0.0
the best agent: 0, best agent cum rewards: -131.6638810247526
33
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.2401436450262
avg cum rews: -132.2401436450262, std: 0.0
the best agent: 0, best agent cum rewards: -132.2401436450262
34
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.37403968101776
avg cum rews: -131.37403968101776, std: 0.0
the best agent: 0, best agent cum rewards: -131.37403968101776
35
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.3549016859612
avg cum rews: -131.3549016859612, std: 0.0
the best agent: 0, best agent cum rewards: -131.3549016859612
36
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.87419612949583
avg cum rews: -130.87419612949583, std: 0.0
the best agent: 0, best agent cum rewards: -130.87419612949583
37
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.58599292203007
avg cum rews: -130.58599292203007, std: 0.0
the best agent: 0, best agent cum rewards: -130.58599292203007
38
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.72651159147492
avg cum rews: -130.72651159147492, std: 0.0
the best agent: 0, best agent cum rewards: -130.72651159147492
39
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -130.75129327861106
avg cum rews: -130.75129327861106, std: 0.0
the best agent: 0, best agent cum rewards: -130.75129327861106
40
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.4242647879494
avg cum rews: -131.4242647879494, std: 0.0
the best agent: 0, best agent cum rewards: -131.4242647879494
41
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -131.85197900623754
avg cum rews: -131.85197900623754, std: 0.0
the best agent: 0, best agent cum rewards: -131.85197900623754
42
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -132.47926117280568
avg cum rews: -132.47926117280568, std: 0.0
the best agent: 0, best agent cum rewards: -132.47926117280568
43
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -133.10701760238248
avg cum rews: -133.10701760238248, std: 0.0
the best agent: 0, best agent cum rewards: -133.10701760238248
44
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -133.0335766243311
avg cum rews: -133.0335766243311, std: 0.0
the best agent: 0, best agent cum rewards: -133.0335766243311
45
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -133.22674743068092
avg cum rews: -133.22674743068092, std: 0.0
the best agent: 0, best agent cum rewards: -133.22674743068092
46
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -264.3686992409462
avg cum rews: -264.3686992409462, std: 0.0
the best agent: 0, best agent cum rewards: -264.3686992409462
47
---------------------------------
Searching policies using Value Function Search
avg return on 3 trajectories of agent0: -267.2961010363979
avg cum rews: -267.2961010363979, std: 0.0
the best agent: 0, best agent cum rewards: -267.2961010363979
Average distance of random agents to nearest neighbors: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Time taken for each iteration: [7.813027620315552, 15.67572808265686, 23.49550747871399, 31.302428007125854, 39.11714291572571, 46.932374000549316, 54.74743843078613, 62.58446979522705, 70.40042996406555, 78.2156937122345, 86.02881336212158, 93.8398220539093, 101.6561164855957, 109.4758186340332, 117.29753947257996, 125.10298681259155, 132.91723227500916, 140.71650958061218, 148.53161311149597, 156.33657503128052, 164.14848256111145, 171.95987462997437, 179.78447079658508, 187.59671545028687, 195.41161441802979, 202.91793417930603, 210.42448592185974, 217.92851328849792, 225.4297525882721, 232.92851495742798, 240.43273544311523, 247.94025230407715, 255.44010019302368, 262.93997526168823, 270.44904112815857, 277.95127272605896, 285.4589216709137, 292.9676458835602, 300.46778297424316, 307.9772980213165, 315.4686906337738, 322.9841389656067, 330.4833285808563, 337.9828681945801, 345.49104022979736, 353.00155234336853, 360.51321148872375, 368.00560760498047]
