No devices were found
Setting seed -  0
---------------------------------
Environment created
Box(-1.0, 1.0, (4,), float32) Box(-inf, inf, (16,), float64)
Starting training from scratch
Starting evaluation
1
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.017743269709626416
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
2
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.022488176126556566
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
3
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.03300469361854006
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
4
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02518306275384145
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
5
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.025656716323074917
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
6
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.027525935576054966
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
7
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.024238156399650695
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
8
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.028197275087188372
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
9
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.025136643679565286
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
10
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.023553671868814574
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
11
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.0287969751919466
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
12
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02595994427323553
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -50.0, std: 0.0
the best agent: 19, best agent cum rewards: -50.0
13
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.025658054989138944
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -44.666666666666664
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -50.0
avg return on 3 trajectories of agent10: -50.0
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -49.0
avg return on 3 trajectories of agent18: -49.333333333333336
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -49.65, std: 1.171300322054274
the best agent: 3, best agent cum rewards: -44.666666666666664
14
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02976742418294796
avg return on 3 trajectories of agent0: -50.0
avg return on 3 trajectories of agent1: -44.666666666666664
avg return on 3 trajectories of agent2: -41.666666666666664
avg return on 3 trajectories of agent3: -40.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -50.0
avg return on 3 trajectories of agent9: -34.333333333333336
avg return on 3 trajectories of agent10: -33.0
avg return on 3 trajectories of agent11: -37.0
avg return on 3 trajectories of agent12: -50.0
avg return on 3 trajectories of agent13: -50.0
avg return on 3 trajectories of agent14: -50.0
avg return on 3 trajectories of agent15: -50.0
avg return on 3 trajectories of agent16: -50.0
avg return on 3 trajectories of agent17: -50.0
avg return on 3 trajectories of agent18: -50.0
avg return on 3 trajectories of agent19: -50.0
avg cum rews: -46.53333333333333, std: 5.748816303282694
the best agent: 10, best agent cum rewards: -33.0
15
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.028300532267918944
avg return on 3 trajectories of agent0: -49.666666666666664
avg return on 3 trajectories of agent1: -50.0
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -50.0
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -46.333333333333336
avg return on 3 trajectories of agent9: -46.333333333333336
avg return on 3 trajectories of agent10: -49.333333333333336
avg return on 3 trajectories of agent11: -50.0
avg return on 3 trajectories of agent12: -38.333333333333336
avg return on 3 trajectories of agent13: -38.666666666666664
avg return on 3 trajectories of agent14: -37.0
avg return on 3 trajectories of agent15: -44.666666666666664
avg return on 3 trajectories of agent16: -39.666666666666664
avg return on 3 trajectories of agent17: -30.666666666666668
avg return on 3 trajectories of agent18: -27.666666666666668
avg return on 3 trajectories of agent19: -31.0
avg cum rews: -43.966666666666654, std: 7.4221141043356225
the best agent: 18, best agent cum rewards: -27.666666666666668
16
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.028741687689933054
avg return on 3 trajectories of agent0: -39.0
avg return on 3 trajectories of agent1: -42.333333333333336
avg return on 3 trajectories of agent2: -36.333333333333336
avg return on 3 trajectories of agent3: -35.333333333333336
avg return on 3 trajectories of agent4: -38.666666666666664
avg return on 3 trajectories of agent5: -39.0
avg return on 3 trajectories of agent6: -44.0
avg return on 3 trajectories of agent7: -46.0
avg return on 3 trajectories of agent8: -40.0
avg return on 3 trajectories of agent9: -39.0
avg return on 3 trajectories of agent10: -37.0
avg return on 3 trajectories of agent11: -36.0
avg return on 3 trajectories of agent12: -39.666666666666664
avg return on 3 trajectories of agent13: -41.666666666666664
avg return on 3 trajectories of agent14: -43.333333333333336
avg return on 3 trajectories of agent15: -44.0
avg return on 3 trajectories of agent16: -40.0
avg return on 3 trajectories of agent17: -36.0
avg return on 3 trajectories of agent18: -33.333333333333336
avg return on 3 trajectories of agent19: -31.666666666666668
avg cum rews: -39.11666666666667, std: 3.651902091908939
the best agent: 19, best agent cum rewards: -31.666666666666668
17
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.022837611765381568
avg return on 3 trajectories of agent0: -27.666666666666668
avg return on 3 trajectories of agent1: -26.333333333333332
avg return on 3 trajectories of agent2: -27.666666666666668
avg return on 3 trajectories of agent3: -30.666666666666668
avg return on 3 trajectories of agent4: -27.666666666666668
avg return on 3 trajectories of agent5: -27.666666666666668
avg return on 3 trajectories of agent6: -29.666666666666668
avg return on 3 trajectories of agent7: -34.333333333333336
avg return on 3 trajectories of agent8: -29.666666666666668
avg return on 3 trajectories of agent9: -24.333333333333332
avg return on 3 trajectories of agent10: -24.0
avg return on 3 trajectories of agent11: -25.0
avg return on 3 trajectories of agent12: -25.666666666666668
avg return on 3 trajectories of agent13: -24.666666666666668
avg return on 3 trajectories of agent14: -26.666666666666668
avg return on 3 trajectories of agent15: -28.333333333333332
avg return on 3 trajectories of agent16: -29.0
avg return on 3 trajectories of agent17: -28.666666666666668
avg return on 3 trajectories of agent18: -30.666666666666668
avg return on 3 trajectories of agent19: -31.666666666666668
avg cum rews: -28.0, std: 2.597006824104328
the best agent: 10, best agent cum rewards: -24.0
18
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02517090001337726
avg return on 3 trajectories of agent0: -23.333333333333332
avg return on 3 trajectories of agent1: -24.666666666666668
avg return on 3 trajectories of agent2: -28.333333333333332
avg return on 3 trajectories of agent3: -41.0
avg return on 3 trajectories of agent4: -32.0
avg return on 3 trajectories of agent5: -35.0
avg return on 3 trajectories of agent6: -44.0
avg return on 3 trajectories of agent7: -48.666666666666664
avg return on 3 trajectories of agent8: -28.666666666666668
avg return on 3 trajectories of agent9: -26.666666666666668
avg return on 3 trajectories of agent10: -24.666666666666668
avg return on 3 trajectories of agent11: -26.333333333333332
avg return on 3 trajectories of agent12: -30.333333333333332
avg return on 3 trajectories of agent13: -29.666666666666668
avg return on 3 trajectories of agent14: -30.333333333333332
avg return on 3 trajectories of agent15: -38.666666666666664
avg return on 3 trajectories of agent16: -25.666666666666668
avg return on 3 trajectories of agent17: -27.0
avg return on 3 trajectories of agent18: -33.666666666666664
avg return on 3 trajectories of agent19: -38.0
avg cum rews: -31.833333333333325, std: 6.849574196011504
the best agent: 0, best agent cum rewards: -23.333333333333332
19
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.028614364487135342
avg return on 3 trajectories of agent0: -23.666666666666668
avg return on 3 trajectories of agent1: -34.333333333333336
avg return on 3 trajectories of agent2: -38.333333333333336
avg return on 3 trajectories of agent3: -42.666666666666664
avg return on 3 trajectories of agent4: -22.333333333333332
avg return on 3 trajectories of agent5: -30.0
avg return on 3 trajectories of agent6: -29.333333333333332
avg return on 3 trajectories of agent7: -29.666666666666668
avg return on 3 trajectories of agent8: -25.0
avg return on 3 trajectories of agent9: -24.666666666666668
avg return on 3 trajectories of agent10: -25.0
avg return on 3 trajectories of agent11: -27.666666666666668
avg return on 3 trajectories of agent12: -29.333333333333332
avg return on 3 trajectories of agent13: -29.666666666666668
avg return on 3 trajectories of agent14: -35.333333333333336
avg return on 3 trajectories of agent15: -39.0
avg return on 3 trajectories of agent16: -28.333333333333332
avg return on 3 trajectories of agent17: -30.333333333333332
avg return on 3 trajectories of agent18: -34.666666666666664
avg return on 3 trajectories of agent19: -38.333333333333336
avg cum rews: -30.883333333333336, std: 5.570033911726012
the best agent: 4, best agent cum rewards: -22.333333333333332
20
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02505028945882779
avg return on 3 trajectories of agent0: -28.0
avg return on 3 trajectories of agent1: -30.666666666666668
avg return on 3 trajectories of agent2: -29.333333333333332
avg return on 3 trajectories of agent3: -35.666666666666664
avg return on 3 trajectories of agent4: -28.333333333333332
avg return on 3 trajectories of agent5: -24.333333333333332
avg return on 3 trajectories of agent6: -28.666666666666668
avg return on 3 trajectories of agent7: -33.0
avg return on 3 trajectories of agent8: -27.333333333333332
avg return on 3 trajectories of agent9: -29.333333333333332
avg return on 3 trajectories of agent10: -25.0
avg return on 3 trajectories of agent11: -30.333333333333332
avg return on 3 trajectories of agent12: -28.0
avg return on 3 trajectories of agent13: -27.0
avg return on 3 trajectories of agent14: -29.0
avg return on 3 trajectories of agent15: -44.0
avg return on 3 trajectories of agent16: -28.0
avg return on 3 trajectories of agent17: -27.333333333333332
avg return on 3 trajectories of agent18: -26.333333333333332
avg return on 3 trajectories of agent19: -26.333333333333332
avg cum rews: -29.300000000000004, std: 4.204362813395945
the best agent: 5, best agent cum rewards: -24.333333333333332
21
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.023757420317592422
avg return on 3 trajectories of agent0: -22.666666666666668
avg return on 3 trajectories of agent1: -26.0
avg return on 3 trajectories of agent2: -31.0
avg return on 3 trajectories of agent3: -33.0
avg return on 3 trajectories of agent4: -27.0
avg return on 3 trajectories of agent5: -24.666666666666668
avg return on 3 trajectories of agent6: -24.0
avg return on 3 trajectories of agent7: -23.666666666666668
avg return on 3 trajectories of agent8: -21.666666666666668
avg return on 3 trajectories of agent9: -29.0
avg return on 3 trajectories of agent10: -35.666666666666664
avg return on 3 trajectories of agent11: -36.666666666666664
avg return on 3 trajectories of agent12: -17.666666666666668
avg return on 3 trajectories of agent13: -18.0
avg return on 3 trajectories of agent14: -22.666666666666668
avg return on 3 trajectories of agent15: -24.333333333333332
avg return on 3 trajectories of agent16: -17.333333333333332
avg return on 3 trajectories of agent17: -16.333333333333332
avg return on 3 trajectories of agent18: -17.0
avg return on 3 trajectories of agent19: -18.333333333333332
avg cum rews: -24.33333333333333, std: 6.051629716218782
the best agent: 17, best agent cum rewards: -16.333333333333332
22
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.030024128870810067
avg return on 3 trajectories of agent0: -20.0
avg return on 3 trajectories of agent1: -24.333333333333332
avg return on 3 trajectories of agent2: -24.333333333333332
avg return on 3 trajectories of agent3: -34.333333333333336
avg return on 3 trajectories of agent4: -22.333333333333332
avg return on 3 trajectories of agent5: -28.0
avg return on 3 trajectories of agent6: -28.333333333333332
avg return on 3 trajectories of agent7: -28.333333333333332
avg return on 3 trajectories of agent8: -19.666666666666668
avg return on 3 trajectories of agent9: -26.0
avg return on 3 trajectories of agent10: -26.0
avg return on 3 trajectories of agent11: -27.0
avg return on 3 trajectories of agent12: -15.333333333333334
avg return on 3 trajectories of agent13: -14.666666666666666
avg return on 3 trajectories of agent14: -14.666666666666666
avg return on 3 trajectories of agent15: -14.666666666666666
avg return on 3 trajectories of agent16: -17.0
avg return on 3 trajectories of agent17: -22.333333333333332
avg return on 3 trajectories of agent18: -30.0
avg return on 3 trajectories of agent19: -38.0
avg cum rews: -23.766666666666666, std: 6.458499997849518
the best agent: 14, best agent cum rewards: -14.666666666666666
23
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.027623238593391664
avg return on 3 trajectories of agent0: -29.0
avg return on 3 trajectories of agent1: -42.333333333333336
avg return on 3 trajectories of agent2: -50.0
avg return on 3 trajectories of agent3: -50.0
avg return on 3 trajectories of agent4: -30.333333333333332
avg return on 3 trajectories of agent5: -50.0
avg return on 3 trajectories of agent6: -50.0
avg return on 3 trajectories of agent7: -50.0
avg return on 3 trajectories of agent8: -25.666666666666668
avg return on 3 trajectories of agent9: -14.333333333333334
avg return on 3 trajectories of agent10: -11.666666666666666
avg return on 3 trajectories of agent11: -19.666666666666668
avg return on 3 trajectories of agent12: -26.666666666666668
avg return on 3 trajectories of agent13: -26.333333333333332
avg return on 3 trajectories of agent14: -26.333333333333332
avg return on 3 trajectories of agent15: -27.666666666666668
avg return on 3 trajectories of agent16: -27.666666666666668
avg return on 3 trajectories of agent17: -28.666666666666668
avg return on 3 trajectories of agent18: -31.333333333333332
avg return on 3 trajectories of agent19: -48.0
avg cum rews: -33.28333333333333, std: 12.290590348356384
the best agent: 10, best agent cum rewards: -11.666666666666666
24
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02521801388303372
avg return on 3 trajectories of agent0: -12.0
avg return on 3 trajectories of agent1: -11.333333333333334
avg return on 3 trajectories of agent2: -10.666666666666666
avg return on 3 trajectories of agent3: -22.0
avg return on 3 trajectories of agent4: -11.333333333333334
avg return on 3 trajectories of agent5: -10.333333333333334
avg return on 3 trajectories of agent6: -10.0
avg return on 3 trajectories of agent7: -20.666666666666668
avg return on 3 trajectories of agent8: -11.333333333333334
avg return on 3 trajectories of agent9: -15.333333333333334
avg return on 3 trajectories of agent10: -21.666666666666668
avg return on 3 trajectories of agent11: -26.333333333333332
avg return on 3 trajectories of agent12: -11.333333333333334
avg return on 3 trajectories of agent13: -9.333333333333334
avg return on 3 trajectories of agent14: -13.333333333333334
avg return on 3 trajectories of agent15: -18.0
avg return on 3 trajectories of agent16: -9.666666666666666
avg return on 3 trajectories of agent17: -15.0
avg return on 3 trajectories of agent18: -17.0
avg return on 3 trajectories of agent19: -23.333333333333332
avg cum rews: -14.999999999999996, std: 5.139174166255811
the best agent: 13, best agent cum rewards: -9.333333333333334
25
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.026081196213370388
avg return on 3 trajectories of agent0: -9.0
avg return on 3 trajectories of agent1: -9.666666666666666
avg return on 3 trajectories of agent2: -20.0
avg return on 3 trajectories of agent3: -19.666666666666668
avg return on 3 trajectories of agent4: -10.0
avg return on 3 trajectories of agent5: -20.333333333333332
avg return on 3 trajectories of agent6: -20.333333333333332
avg return on 3 trajectories of agent7: -20.333333333333332
avg return on 3 trajectories of agent8: -8.666666666666666
avg return on 3 trajectories of agent9: -8.0
avg return on 3 trajectories of agent10: -7.666666666666667
avg return on 3 trajectories of agent11: -7.666666666666667
avg return on 3 trajectories of agent12: -8.333333333333334
avg return on 3 trajectories of agent13: -8.0
avg return on 3 trajectories of agent14: -8.0
avg return on 3 trajectories of agent15: -8.666666666666666
avg return on 3 trajectories of agent16: -10.0
avg return on 3 trajectories of agent17: -7.333333333333333
avg return on 3 trajectories of agent18: -11.333333333333334
avg return on 3 trajectories of agent19: -36.333333333333336
avg cum rews: -12.966666666666665, std: 7.3158124026734805
the best agent: 17, best agent cum rewards: -7.333333333333333
26
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02422106044389035
avg return on 3 trajectories of agent0: -7.333333333333333
avg return on 3 trajectories of agent1: -7.333333333333333
avg return on 3 trajectories of agent2: -7.0
avg return on 3 trajectories of agent3: -7.0
avg return on 3 trajectories of agent4: -7.0
avg return on 3 trajectories of agent5: -7.0
avg return on 3 trajectories of agent6: -6.666666666666667
avg return on 3 trajectories of agent7: -15.666666666666666
avg return on 3 trajectories of agent8: -7.666666666666667
avg return on 3 trajectories of agent9: -8.333333333333334
avg return on 3 trajectories of agent10: -9.333333333333334
avg return on 3 trajectories of agent11: -24.0
avg return on 3 trajectories of agent12: -6.666666666666667
avg return on 3 trajectories of agent13: -7.0
avg return on 3 trajectories of agent14: -14.0
avg return on 3 trajectories of agent15: -25.666666666666668
avg return on 3 trajectories of agent16: -7.0
avg return on 3 trajectories of agent17: -6.333333333333333
avg return on 3 trajectories of agent18: -6.0
avg return on 3 trajectories of agent19: -12.0
avg cum rews: -9.950000000000001, std: 5.577011346192112
the best agent: 18, best agent cum rewards: -6.0
27
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.026671714135354473
avg return on 3 trajectories of agent0: -5.333333333333333
avg return on 3 trajectories of agent1: -5.0
avg return on 3 trajectories of agent2: -5.0
avg return on 3 trajectories of agent3: -7.333333333333333
avg return on 3 trajectories of agent4: -5.666666666666667
avg return on 3 trajectories of agent5: -5.666666666666667
avg return on 3 trajectories of agent6: -6.0
avg return on 3 trajectories of agent7: -6.333333333333333
avg return on 3 trajectories of agent8: -5.0
avg return on 3 trajectories of agent9: -5.666666666666667
avg return on 3 trajectories of agent10: -6.0
avg return on 3 trajectories of agent11: -6.333333333333333
avg return on 3 trajectories of agent12: -5.333333333333333
avg return on 3 trajectories of agent13: -5.333333333333333
avg return on 3 trajectories of agent14: -6.0
avg return on 3 trajectories of agent15: -7.333333333333333
avg return on 3 trajectories of agent16: -5.333333333333333
avg return on 3 trajectories of agent17: -5.333333333333333
avg return on 3 trajectories of agent18: -5.666666666666667
avg return on 3 trajectories of agent19: -7.0
avg cum rews: -5.833333333333333, std: 0.7031674369909662
the best agent: 1, best agent cum rewards: -5.0
28
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.028790762401309743
avg return on 3 trajectories of agent0: -6.0
avg return on 3 trajectories of agent1: -5.666666666666667
avg return on 3 trajectories of agent2: -5.666666666666667
avg return on 3 trajectories of agent3: -7.0
avg return on 3 trajectories of agent4: -6.0
avg return on 3 trajectories of agent5: -6.0
avg return on 3 trajectories of agent6: -6.0
avg return on 3 trajectories of agent7: -5.666666666666667
avg return on 3 trajectories of agent8: -5.666666666666667
avg return on 3 trajectories of agent9: -6.0
avg return on 3 trajectories of agent10: -6.666666666666667
avg return on 3 trajectories of agent11: -7.0
avg return on 3 trajectories of agent12: -5.333333333333333
avg return on 3 trajectories of agent13: -5.666666666666667
avg return on 3 trajectories of agent14: -5.666666666666667
avg return on 3 trajectories of agent15: -6.0
avg return on 3 trajectories of agent16: -5.333333333333333
avg return on 3 trajectories of agent17: -5.333333333333333
avg return on 3 trajectories of agent18: -5.0
avg return on 3 trajectories of agent19: -5.333333333333333
avg cum rews: -5.85, std: 0.5214829282387339
the best agent: 18, best agent cum rewards: -5.0
29
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.026798809915719653
avg return on 3 trajectories of agent0: -4.333333333333333
avg return on 3 trajectories of agent1: -4.666666666666667
avg return on 3 trajectories of agent2: -5.0
avg return on 3 trajectories of agent3: -5.333333333333333
avg return on 3 trajectories of agent4: -5.0
avg return on 3 trajectories of agent5: -5.333333333333333
avg return on 3 trajectories of agent6: -6.0
avg return on 3 trajectories of agent7: -6.333333333333333
avg return on 3 trajectories of agent8: -4.333333333333333
avg return on 3 trajectories of agent9: -4.333333333333333
avg return on 3 trajectories of agent10: -4.0
avg return on 3 trajectories of agent11: -4.0
avg return on 3 trajectories of agent12: -4.0
avg return on 3 trajectories of agent13: -4.333333333333333
avg return on 3 trajectories of agent14: -4.666666666666667
avg return on 3 trajectories of agent15: -5.333333333333333
avg return on 3 trajectories of agent16: -4.0
avg return on 3 trajectories of agent17: -4.666666666666667
avg return on 3 trajectories of agent18: -6.0
avg return on 3 trajectories of agent19: -6.666666666666667
avg cum rews: -4.916666666666667, std: 0.8019074482479606
the best agent: 16, best agent cum rewards: -4.0
30
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.030089232701399866
avg return on 3 trajectories of agent0: -3.6666666666666665
avg return on 3 trajectories of agent1: -4.0
avg return on 3 trajectories of agent2: -4.333333333333333
avg return on 3 trajectories of agent3: -4.333333333333333
avg return on 3 trajectories of agent4: -3.6666666666666665
avg return on 3 trajectories of agent5: -3.6666666666666665
avg return on 3 trajectories of agent6: -3.6666666666666665
avg return on 3 trajectories of agent7: -3.6666666666666665
avg return on 3 trajectories of agent8: -3.6666666666666665
avg return on 3 trajectories of agent9: -4.0
avg return on 3 trajectories of agent10: -4.333333333333333
avg return on 3 trajectories of agent11: -4.666666666666667
avg return on 3 trajectories of agent12: -3.6666666666666665
avg return on 3 trajectories of agent13: -3.6666666666666665
avg return on 3 trajectories of agent14: -3.6666666666666665
avg return on 3 trajectories of agent15: -3.3333333333333335
avg return on 3 trajectories of agent16: -3.6666666666666665
avg return on 3 trajectories of agent17: -3.6666666666666665
avg return on 3 trajectories of agent18: -3.3333333333333335
avg return on 3 trajectories of agent19: -3.3333333333333335
avg cum rews: -3.8, std: 0.3559026084010437
the best agent: 19, best agent cum rewards: -3.3333333333333335
31
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.023796157779295335
avg return on 3 trajectories of agent0: -3.3333333333333335
avg return on 3 trajectories of agent1: -3.6666666666666665
avg return on 3 trajectories of agent2: -4.0
avg return on 3 trajectories of agent3: -4.0
avg return on 3 trajectories of agent4: -3.3333333333333335
avg return on 3 trajectories of agent5: -3.6666666666666665
avg return on 3 trajectories of agent6: -3.6666666666666665
avg return on 3 trajectories of agent7: -4.0
avg return on 3 trajectories of agent8: -3.3333333333333335
avg return on 3 trajectories of agent9: -4.0
avg return on 3 trajectories of agent10: -4.0
avg return on 3 trajectories of agent11: -4.0
avg return on 3 trajectories of agent12: -3.3333333333333335
avg return on 3 trajectories of agent13: -3.6666666666666665
avg return on 3 trajectories of agent14: -3.6666666666666665
avg return on 3 trajectories of agent15: -14.333333333333334
avg return on 3 trajectories of agent16: -3.3333333333333335
avg return on 3 trajectories of agent17: -3.3333333333333335
avg return on 3 trajectories of agent18: -3.6666666666666665
avg return on 3 trajectories of agent19: -11.0
avg cum rews: -4.566666666666666, std: 2.763049844726737
the best agent: 16, best agent cum rewards: -3.3333333333333335
32
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.025419505713820307
avg return on 3 trajectories of agent0: -4.0
avg return on 3 trajectories of agent1: -4.0
avg return on 3 trajectories of agent2: -4.0
avg return on 3 trajectories of agent3: -4.0
avg return on 3 trajectories of agent4: -3.3333333333333335
avg return on 3 trajectories of agent5: -3.0
avg return on 3 trajectories of agent6: -3.0
avg return on 3 trajectories of agent7: -3.0
avg return on 3 trajectories of agent8: -4.0
avg return on 3 trajectories of agent9: -4.0
avg return on 3 trajectories of agent10: -4.333333333333333
avg return on 3 trajectories of agent11: -4.333333333333333
avg return on 3 trajectories of agent12: -3.3333333333333335
avg return on 3 trajectories of agent13: -3.3333333333333335
avg return on 3 trajectories of agent14: -3.3333333333333335
avg return on 3 trajectories of agent15: -3.3333333333333335
avg return on 3 trajectories of agent16: -3.3333333333333335
avg return on 3 trajectories of agent17: -3.6666666666666665
avg return on 3 trajectories of agent18: -4.0
avg return on 3 trajectories of agent19: -4.0
avg cum rews: -3.6666666666666665, std: 0.4346134936801765
the best agent: 5, best agent cum rewards: -3.0
33
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.026801862312095526
avg return on 3 trajectories of agent0: -3.6666666666666665
avg return on 3 trajectories of agent1: -3.3333333333333335
avg return on 3 trajectories of agent2: -3.3333333333333335
avg return on 3 trajectories of agent3: -3.3333333333333335
avg return on 3 trajectories of agent4: -3.3333333333333335
avg return on 3 trajectories of agent5: -3.0
avg return on 3 trajectories of agent6: -3.0
avg return on 3 trajectories of agent7: -3.3333333333333335
avg return on 3 trajectories of agent8: -3.3333333333333335
avg return on 3 trajectories of agent9: -3.3333333333333335
avg return on 3 trajectories of agent10: -3.6666666666666665
avg return on 3 trajectories of agent11: -3.6666666666666665
avg return on 3 trajectories of agent12: -3.0
avg return on 3 trajectories of agent13: -3.0
avg return on 3 trajectories of agent14: -3.3333333333333335
avg return on 3 trajectories of agent15: -3.3333333333333335
avg return on 3 trajectories of agent16: -3.0
avg return on 3 trajectories of agent17: -3.0
avg return on 3 trajectories of agent18: -3.0
avg return on 3 trajectories of agent19: -3.0
avg cum rews: -3.25, std: 0.2327373340628157
the best agent: 19, best agent cum rewards: -3.0
34
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02432665270351066
avg return on 3 trajectories of agent0: -3.3333333333333335
avg return on 3 trajectories of agent1: -3.6666666666666665
avg return on 3 trajectories of agent2: -4.0
avg return on 3 trajectories of agent3: -4.333333333333333
avg return on 3 trajectories of agent4: -3.3333333333333335
avg return on 3 trajectories of agent5: -3.3333333333333335
avg return on 3 trajectories of agent6: -3.6666666666666665
avg return on 3 trajectories of agent7: -3.6666666666666665
avg return on 3 trajectories of agent8: -3.3333333333333335
avg return on 3 trajectories of agent9: -3.3333333333333335
avg return on 3 trajectories of agent10: -3.0
avg return on 3 trajectories of agent11: -3.0
avg return on 3 trajectories of agent12: -3.3333333333333335
avg return on 3 trajectories of agent13: -3.0
avg return on 3 trajectories of agent14: -3.0
avg return on 3 trajectories of agent15: -3.0
avg return on 3 trajectories of agent16: -3.3333333333333335
avg return on 3 trajectories of agent17: -3.0
avg return on 3 trajectories of agent18: -2.6666666666666665
avg return on 3 trajectories of agent19: -2.6666666666666665
avg cum rews: -3.3, std: 0.40688518719112343
the best agent: 19, best agent cum rewards: -2.6666666666666665
35
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02402903931063799
avg return on 3 trajectories of agent0: -3.0
avg return on 3 trajectories of agent1: -2.6666666666666665
avg return on 3 trajectories of agent2: -2.6666666666666665
avg return on 3 trajectories of agent3: -2.6666666666666665
avg return on 3 trajectories of agent4: -3.0
avg return on 3 trajectories of agent5: -3.0
avg return on 3 trajectories of agent6: -3.0
avg return on 3 trajectories of agent7: -3.0
avg return on 3 trajectories of agent8: -3.0
avg return on 3 trajectories of agent9: -2.6666666666666665
avg return on 3 trajectories of agent10: -2.6666666666666665
avg return on 3 trajectories of agent11: -2.6666666666666665
avg return on 3 trajectories of agent12: -3.0
avg return on 3 trajectories of agent13: -2.6666666666666665
avg return on 3 trajectories of agent14: -2.6666666666666665
avg return on 3 trajectories of agent15: -2.6666666666666665
avg return on 3 trajectories of agent16: -3.0
avg return on 3 trajectories of agent17: -2.6666666666666665
avg return on 3 trajectories of agent18: -2.6666666666666665
avg return on 3 trajectories of agent19: -2.6666666666666665
avg cum rews: -2.8, std: 0.16329931618554527
the best agent: 19, best agent cum rewards: -2.6666666666666665
36
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02444936528148566
avg return on 3 trajectories of agent0: -2.3333333333333335
avg return on 3 trajectories of agent1: -2.6666666666666665
avg return on 3 trajectories of agent2: -2.6666666666666665
avg return on 3 trajectories of agent3: -3.3333333333333335
avg return on 3 trajectories of agent4: -2.3333333333333335
avg return on 3 trajectories of agent5: -2.3333333333333335
avg return on 3 trajectories of agent6: -2.6666666666666665
avg return on 3 trajectories of agent7: -2.6666666666666665
avg return on 3 trajectories of agent8: -2.6666666666666665
avg return on 3 trajectories of agent9: -2.6666666666666665
avg return on 3 trajectories of agent10: -2.6666666666666665
avg return on 3 trajectories of agent11: -2.6666666666666665
avg return on 3 trajectories of agent12: -2.6666666666666665
avg return on 3 trajectories of agent13: -2.6666666666666665
avg return on 3 trajectories of agent14: -2.3333333333333335
avg return on 3 trajectories of agent15: -3.0
avg return on 3 trajectories of agent16: -2.6666666666666665
avg return on 3 trajectories of agent17: -2.6666666666666665
avg return on 3 trajectories of agent18: -3.0
avg return on 3 trajectories of agent19: -3.0
avg cum rews: -2.6833333333333327, std: 0.24664414311581237
the best agent: 4, best agent cum rewards: -2.3333333333333335
37
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.023385562881156518
avg return on 3 trajectories of agent0: -2.6666666666666665
avg return on 3 trajectories of agent1: -2.6666666666666665
avg return on 3 trajectories of agent2: -2.6666666666666665
avg return on 3 trajectories of agent3: -3.0
avg return on 3 trajectories of agent4: -2.6666666666666665
avg return on 3 trajectories of agent5: -3.0
avg return on 3 trajectories of agent6: -3.0
avg return on 3 trajectories of agent7: -3.3333333333333335
avg return on 3 trajectories of agent8: -2.6666666666666665
avg return on 3 trajectories of agent9: -2.3333333333333335
avg return on 3 trajectories of agent10: -2.3333333333333335
avg return on 3 trajectories of agent11: -2.6666666666666665
avg return on 3 trajectories of agent12: -2.3333333333333335
avg return on 3 trajectories of agent13: -2.6666666666666665
avg return on 3 trajectories of agent14: -2.3333333333333335
avg return on 3 trajectories of agent15: -2.6666666666666665
avg return on 3 trajectories of agent16: -2.3333333333333335
avg return on 3 trajectories of agent17: -2.6666666666666665
avg return on 3 trajectories of agent18: -2.3333333333333335
avg return on 3 trajectories of agent19: -2.6666666666666665
avg cum rews: -2.65, std: 0.2682246156571847
the best agent: 18, best agent cum rewards: -2.3333333333333335
38
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02533701373694771
avg return on 3 trajectories of agent0: -2.6666666666666665
avg return on 3 trajectories of agent1: -3.0
avg return on 3 trajectories of agent2: -3.3333333333333335
avg return on 3 trajectories of agent3: -3.3333333333333335
avg return on 3 trajectories of agent4: -2.6666666666666665
avg return on 3 trajectories of agent5: -2.6666666666666665
avg return on 3 trajectories of agent6: -2.6666666666666665
avg return on 3 trajectories of agent7: -2.6666666666666665
avg return on 3 trajectories of agent8: -2.6666666666666665
avg return on 3 trajectories of agent9: -2.6666666666666665
avg return on 3 trajectories of agent10: -2.6666666666666665
avg return on 3 trajectories of agent11: -2.6666666666666665
avg return on 3 trajectories of agent12: -2.6666666666666665
avg return on 3 trajectories of agent13: -2.6666666666666665
avg return on 3 trajectories of agent14: -2.3333333333333335
avg return on 3 trajectories of agent15: -2.3333333333333335
avg return on 3 trajectories of agent16: -2.6666666666666665
avg return on 3 trajectories of agent17: -2.6666666666666665
avg return on 3 trajectories of agent18: -2.6666666666666665
avg return on 3 trajectories of agent19: -2.6666666666666665
avg cum rews: -2.716666666666666, std: 0.24209731743889923
the best agent: 14, best agent cum rewards: -2.3333333333333335
39
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02542521404348285
avg return on 3 trajectories of agent0: -2.6666666666666665
avg return on 3 trajectories of agent1: -2.6666666666666665
avg return on 3 trajectories of agent2: -3.0
avg return on 3 trajectories of agent3: -3.3333333333333335
avg return on 3 trajectories of agent4: -2.6666666666666665
avg return on 3 trajectories of agent5: -2.3333333333333335
avg return on 3 trajectories of agent6: -2.3333333333333335
avg return on 3 trajectories of agent7: -2.6666666666666665
avg return on 3 trajectories of agent8: -2.3333333333333335
avg return on 3 trajectories of agent9: -2.3333333333333335
avg return on 3 trajectories of agent10: -2.3333333333333335
avg return on 3 trajectories of agent11: -2.3333333333333335
avg return on 3 trajectories of agent12: -2.6666666666666665
avg return on 3 trajectories of agent13: -2.6666666666666665
avg return on 3 trajectories of agent14: -3.0
avg return on 3 trajectories of agent15: -3.0
avg return on 3 trajectories of agent16: -2.3333333333333335
avg return on 3 trajectories of agent17: -2.3333333333333335
avg return on 3 trajectories of agent18: -2.3333333333333335
avg return on 3 trajectories of agent19: -2.6666666666666665
avg cum rews: -2.6, std: 0.29059326290271154
the best agent: 18, best agent cum rewards: -2.3333333333333335
40
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02426421500405531
avg return on 3 trajectories of agent0: -2.6666666666666665
avg return on 3 trajectories of agent1: -2.3333333333333335
avg return on 3 trajectories of agent2: -2.3333333333333335
avg return on 3 trajectories of agent3: -2.3333333333333335
avg return on 3 trajectories of agent4: -2.6666666666666665
avg return on 3 trajectories of agent5: -2.6666666666666665
avg return on 3 trajectories of agent6: -2.6666666666666665
avg return on 3 trajectories of agent7: -2.6666666666666665
avg return on 3 trajectories of agent8: -2.6666666666666665
avg return on 3 trajectories of agent9: -2.6666666666666665
avg return on 3 trajectories of agent10: -3.0
avg return on 3 trajectories of agent11: -3.3333333333333335
avg return on 3 trajectories of agent12: -2.3333333333333335
avg return on 3 trajectories of agent13: -2.3333333333333335
avg return on 3 trajectories of agent14: -2.3333333333333335
avg return on 3 trajectories of agent15: -2.3333333333333335
avg return on 3 trajectories of agent16: -2.6666666666666665
avg return on 3 trajectories of agent17: -2.3333333333333335
avg return on 3 trajectories of agent18: -2.3333333333333335
avg return on 3 trajectories of agent19: -2.3333333333333335
avg cum rews: -2.55, std: 0.26404965862924756
the best agent: 19, best agent cum rewards: -2.3333333333333335
41
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02144989286678282
avg return on 3 trajectories of agent0: -2.3333333333333335
avg return on 3 trajectories of agent1: -2.3333333333333335
avg return on 3 trajectories of agent2: -2.3333333333333335
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.3333333333333335
avg return on 3 trajectories of agent5: -2.3333333333333335
avg return on 3 trajectories of agent6: -2.3333333333333335
avg return on 3 trajectories of agent7: -2.3333333333333335
avg return on 3 trajectories of agent8: -2.3333333333333335
avg return on 3 trajectories of agent9: -2.3333333333333335
avg return on 3 trajectories of agent10: -2.3333333333333335
avg return on 3 trajectories of agent11: -2.3333333333333335
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.3333333333333335
avg return on 3 trajectories of agent16: -2.3333333333333335
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.3333333333333335
avg return on 3 trajectories of agent19: -2.3333333333333335
avg cum rews: -2.2500000000000004, std: 0.14433756729740652
the best agent: 17, best agent cum rewards: -2.0
42
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.019812989122368264
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.3333333333333335
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.3333333333333335
avg return on 3 trajectories of agent19: -2.3333333333333335
avg cum rews: -2.0500000000000003, std: 0.1190238071423809
the best agent: 16, best agent cum rewards: -2.0
43
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02164556187028039
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.3333333333333335
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0166666666666666, std: 0.07264831572567793
the best agent: 19, best agent cum rewards: -2.0
44
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021893515808602385
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.3333333333333335
avg return on 3 trajectories of agent15: -2.6666666666666665
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.3333333333333335
avg cum rews: -2.066666666666667, std: 0.16996731711975951
the best agent: 18, best agent cum rewards: -2.0
45
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021100593107869253
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.3333333333333335
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.3333333333333335
avg return on 3 trajectories of agent7: -2.3333333333333335
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.05, std: 0.11902380714238091
the best agent: 19, best agent cum rewards: -2.0
46
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.020907434266502024
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
47
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.019830593015276662
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.3333333333333335
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0166666666666666, std: 0.07264831572567793
the best agent: 19, best agent cum rewards: -2.0
48
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021295371932633388
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.3333333333333335
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0166666666666666, std: 0.07264831572567793
the best agent: 19, best agent cum rewards: -2.0
49
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021741033954802865
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
50
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.0222022906613717
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
51
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.022109219412725034
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.3333333333333335
avg return on 3 trajectories of agent6: -2.3333333333333335
avg return on 3 trajectories of agent7: -2.6666666666666665
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.3333333333333335
avg return on 3 trajectories of agent14: -2.6666666666666665
avg return on 3 trajectories of agent15: -3.3333333333333335
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.6666666666666665
avg return on 3 trajectories of agent19: -3.0
avg cum rews: -2.2666666666666666, std: 0.38873012632302006
the best agent: 16, best agent cum rewards: -2.0
52
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02139884509852848
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.3333333333333335
avg return on 3 trajectories of agent15: -2.3333333333333335
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0333333333333337, std: 0.10000000000000006
the best agent: 19, best agent cum rewards: -2.0
53
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02234021143847829
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.3333333333333335
avg return on 3 trajectories of agent6: -2.3333333333333335
avg return on 3 trajectories of agent7: -2.6666666666666665
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.3333333333333335
avg return on 3 trajectories of agent15: -2.3333333333333335
avg return on 3 trajectories of agent16: -2.3333333333333335
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.3333333333333335
avg cum rews: -2.1333333333333337, std: 0.19436506316151006
the best agent: 18, best agent cum rewards: -2.0
54
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02185190924644314
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.3333333333333335
avg return on 3 trajectories of agent10: -2.3333333333333335
avg return on 3 trajectories of agent11: -2.3333333333333335
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.05, std: 0.11902380714238091
the best agent: 19, best agent cum rewards: -2.0
55
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.0223470432102222
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
56
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.01952277001318129
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
57
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.022368918275006065
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
58
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.019032077735671036
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.3333333333333335
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0166666666666666, std: 0.07264831572567793
the best agent: 19, best agent cum rewards: -2.0
59
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.01901461890100687
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.3333333333333335
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0166666666666666, std: 0.07264831572567793
the best agent: 19, best agent cum rewards: -2.0
60
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.018460248262505335
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
61
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.022057051170698127
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
62
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021351283131809014
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
63
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02228437462368335
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.3333333333333335
avg return on 3 trajectories of agent3: -2.6666666666666665
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.6666666666666665
avg return on 3 trajectories of agent7: -3.3333333333333335
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.15, std: 0.341158158174312
the best agent: 19, best agent cum rewards: -2.0
64
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021280192431617412
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.3333333333333335
avg return on 3 trajectories of agent2: -2.6666666666666665
avg return on 3 trajectories of agent3: -3.3333333333333335
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.3333333333333335
avg return on 3 trajectories of agent7: -2.3333333333333335
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.15, std: 0.32446537223219646
the best agent: 19, best agent cum rewards: -2.0
65
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.023574203149607297
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.6666666666666665
avg return on 3 trajectories of agent11: -3.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.083333333333333, std: 0.2554951619459315
the best agent: 19, best agent cum rewards: -2.0
66
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021128742061210425
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
67
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.019640871762867017
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
68
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021819024177199036
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
69
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.018776936153700994
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
70
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.023197406099223637
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
71
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02246342844178946
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
72
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021348525005595656
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
73
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02078249455773592
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
74
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.020045135595007594
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
75
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02314875528627992
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
76
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02046134143062219
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.3333333333333335
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0166666666666666, std: 0.07264831572567793
the best agent: 19, best agent cum rewards: -2.0
77
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.025753137185484838
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
78
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02210827119277377
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
79
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.020560603747722696
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
80
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.023580131324314746
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.3333333333333335
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0166666666666666, std: 0.07264831572567793
the best agent: 19, best agent cum rewards: -2.0
81
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.022852870947557564
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
82
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02108097282563206
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
83
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.022349825436003383
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
84
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.017280677584580604
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
85
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02372895590093697
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.3333333333333335
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.3333333333333335
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0333333333333337, std: 0.10000000000000006
the best agent: 19, best agent cum rewards: -2.0
86
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.020659733347090897
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
87
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.01967730475563551
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
88
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021686784405266855
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.3333333333333335
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.3333333333333335
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0333333333333337, std: 0.10000000000000006
the best agent: 19, best agent cum rewards: -2.0
89
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.020706869278575946
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.3333333333333335
avg return on 3 trajectories of agent10: -2.3333333333333335
avg return on 3 trajectories of agent11: -2.3333333333333335
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.05, std: 0.11902380714238091
the best agent: 19, best agent cum rewards: -2.0
90
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.019855290073644945
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.3333333333333335
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0166666666666666, std: 0.07264831572567793
the best agent: 19, best agent cum rewards: -2.0
91
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02226745523819817
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
92
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.01933562509009209
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
93
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.023698062221588267
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.0, std: 0.0
the best agent: 19, best agent cum rewards: -2.0
94
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.02128808260608383
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.3333333333333335
avg return on 3 trajectories of agent7: -2.6666666666666665
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.05, std: 0.15898986690282424
the best agent: 19, best agent cum rewards: -2.0
95
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.021522860686642453
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.0
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.0
avg return on 3 trajectories of agent7: -2.0
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.3333333333333335
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.3333333333333335
avg cum rews: -2.0333333333333337, std: 0.10000000000000005
the best agent: 18, best agent cum rewards: -2.0
96
---------------------------------
Searching empty space policies
(10, 5512)
(20, 5512)
(20, 5512)
Average distance of agents to nearest neighbors: 0.019475612680604396
avg return on 3 trajectories of agent0: -2.0
avg return on 3 trajectories of agent1: -2.0
avg return on 3 trajectories of agent2: -2.0
avg return on 3 trajectories of agent3: -2.3333333333333335
avg return on 3 trajectories of agent4: -2.0
avg return on 3 trajectories of agent5: -2.0
avg return on 3 trajectories of agent6: -2.3333333333333335
avg return on 3 trajectories of agent7: -2.3333333333333335
avg return on 3 trajectories of agent8: -2.0
avg return on 3 trajectories of agent9: -2.0
avg return on 3 trajectories of agent10: -2.0
avg return on 3 trajectories of agent11: -2.0
avg return on 3 trajectories of agent12: -2.0
avg return on 3 trajectories of agent13: -2.0
avg return on 3 trajectories of agent14: -2.0
avg return on 3 trajectories of agent15: -2.0
avg return on 3 trajectories of agent16: -2.0
avg return on 3 trajectories of agent17: -2.0
avg return on 3 trajectories of agent18: -2.0
avg return on 3 trajectories of agent19: -2.0
avg cum rews: -2.05, std: 0.11902380714238091
the best agent: 19, best agent cum rewards: -2.0
Average distance of random agents to nearest neighbors: [0.017743269709626416, 0.022488176126556566, 0.03300469361854006, 0.02518306275384145, 0.025656716323074917, 0.027525935576054966, 0.024238156399650695, 0.028197275087188372, 0.025136643679565286, 0.023553671868814574, 0.0287969751919466, 0.02595994427323553, 0.025658054989138944, 0.02976742418294796, 0.028300532267918944, 0.028741687689933054, 0.022837611765381568, 0.02517090001337726, 0.028614364487135342, 0.02505028945882779, 0.023757420317592422, 0.030024128870810067, 0.027623238593391664, 0.02521801388303372, 0.026081196213370388, 0.02422106044389035, 0.026671714135354473, 0.028790762401309743, 0.026798809915719653, 0.030089232701399866, 0.023796157779295335, 0.025419505713820307, 0.026801862312095526, 0.02432665270351066, 0.02402903931063799, 0.02444936528148566, 0.023385562881156518, 0.02533701373694771, 0.02542521404348285, 0.02426421500405531, 0.02144989286678282, 0.019812989122368264, 0.02164556187028039, 0.021893515808602385, 0.021100593107869253, 0.020907434266502024, 0.019830593015276662, 0.021295371932633388, 0.021741033954802865, 0.0222022906613717, 0.022109219412725034, 0.02139884509852848, 0.02234021143847829, 0.02185190924644314, 0.0223470432102222, 0.01952277001318129, 0.022368918275006065, 0.019032077735671036, 0.01901461890100687, 0.018460248262505335, 0.022057051170698127, 0.021351283131809014, 0.02228437462368335, 0.021280192431617412, 0.023574203149607297, 0.021128742061210425, 0.019640871762867017, 0.021819024177199036, 0.018776936153700994, 0.023197406099223637, 0.02246342844178946, 0.021348525005595656, 0.02078249455773592, 0.020045135595007594, 0.02314875528627992, 0.02046134143062219, 0.025753137185484838, 0.02210827119277377, 0.020560603747722696, 0.023580131324314746, 0.022852870947557564, 0.02108097282563206, 0.022349825436003383, 0.017280677584580604, 0.02372895590093697, 0.020659733347090897, 0.01967730475563551, 0.021686784405266855, 0.020706869278575946, 0.019855290073644945, 0.02226745523819817, 0.01933562509009209, 0.023698062221588267, 0.02128808260608383, 0.021522860686642453, 0.019475612680604396]
Time taken for each iteration: [18.984001398086548, 32.850632190704346, 46.82022190093994, 60.67591953277588, 74.51611113548279, 89.06979036331177, 103.70362734794617, 118.52767252922058, 133.1221821308136, 148.26165127754211, 163.5063304901123, 178.98362803459167, 194.3378689289093, 209.71214127540588, 225.0159878730774, 244.74541544914246, 264.1712019443512, 279.61289858818054, 295.2515890598297, 310.71058773994446, 326.2263352870941, 341.6268894672394, 361.46960043907166, 377.0041854381561, 392.54997181892395, 408.1924538612366, 423.6758282184601, 439.20248341560364, 454.67682814598083, 470.3870851993561, 490.1981792449951, 510.0249285697937, 525.6922972202301, 541.263552904129, 556.8662827014923, 576.7280914783478, 592.4074001312256, 613.6811220645905, 629.2764096260071, 645.0684473514557, 665.9941847324371, 687.3488771915436, 708.8021364212036, 724.292174577713, 739.706348657608, 759.4789707660675, 774.9943137168884, 790.4972121715546, 810.3506891727448, 830.2060379981995, 850.07723736763, 865.5232880115509, 881.0196545124054, 900.764500617981, 916.156492471695, 931.6385188102722, 951.692129611969, 967.2272748947144, 987.0736041069031, 1007.0389766693115, 1026.8704152107239, 1042.2591812610626, 1057.8533246517181, 1073.31449508667, 1093.0913832187653, 1108.4606788158417, 1124.0361235141754, 1143.6503295898438, 1159.0990040302277, 1179.9164521694183, 1195.33855509758, 1214.8516857624054, 1230.2544479370117, 1245.5997183322906, 1261.0557956695557, 1276.7474191188812, 1292.199615240097, 1307.7672703266144, 1327.6073203086853, 1348.2546582221985, 1364.1483449935913, 1380.6514630317688, 1397.2408447265625, 1413.9198143482208, 1430.7674634456635, 1453.3284327983856, 1469.7664403915405, 1492.4829902648926, 1515.2418911457062, 1531.7047572135925, 1554.5645258426666, 1571.146734237671, 1587.7608597278595, 1609.2256398200989, 1632.224975824356, 1648.8458573818207]
