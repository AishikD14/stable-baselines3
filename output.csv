Log Name, Output Name, Description, JobId, Status
Ant-v5
PPO_empty_space_1, ppo_empty_space_out, PPO_empty_space on 100 iterations
PPO_neighbor_search_random_walk, ppo_neighbor_search_random_walk_out, PPO_neighbor_search_random_walk on 100 iterations
PPO_empty_space_Annoy_1, ppo_empty_space_annoy_out, PPO_empty_space with Annoy on 100 iterations with max_episode_steps=200
PPO_empty_space_Faiss_1, ppo_empty_space_faiss_out, PPO_empty_space with Faiss on 100 iterations with max_episode_steps=200
PPO_empty_space_hnswlib_1, ppo_empty_space_hnswlib_out, PPO_empty_space with hnswlib on 100 iterations with max_episode_steps=200
--------------------
PPO_empty_space_2, ppo_empty_space_out, PPO_empty_space on 100 iterations without max_episode_steps=200; gamma=0.9
PPO_neighbor_search_random_walk_2, ppo_neighbor_search_random_walk_out, PPO_neighbor_search_random_walk on 100 iterations without max_episode_steps=200
PPO_random_search_random_walk_2, ppo_random_search_random_walk_out, PPO_random_search random walk on 100 iterations without max_episode_steps=200
PPO_normal_train_2, ppo_normal_train_out, PPO normal train on 100 iterations without max_episode_steps=200
-------------------
PPO_empty_space_Annoy_2, ppo_empty_space_annoy_out, PPO_empty_space with Annoy on 100 iterations without max_episode_steps=200
PPO_empty_space_Faiss_2, ppo_empty_space_faiss_out, PPO_empty_space with Faiss on 100 iterations without max_episode_steps=200
PPO_empty_space_hnswlib_2, ppo_empty_space_hnswlib_out, PPO_empty_space with hnswlib on 100 iterations without max_episode_steps=200
-----------------
PPO_empty_space_optimal, ppo_empty_space_out, PPO_empty_space on 100 iterations with Xinyu hyperparameters
PPO_neighbor_search_random_walk_optimal, ppo_neighbor_search_random_walk_out, PPO_neighbor_search_random_walk on 100 iterations with Xinyu hyperparameters
PPO_random_search_optimal, ppo_random_search_out, PPO_random_search on 100 iterations with Xinyu hyperparameters
PPO_random_search_empty_space_optimal, ppo_random_search_empty_space_out, PPO_random_search_empty_space on 100 iterations with Xinyu hyperparameters
PPO_random_search_random_walk_optimal, ppo_random_search_random_walk_out, PPO_random_search_random_walk on 100 iterations with Xinyu hyperparameters
----------------
PPO_empty_space_optimal_5M_1, ppo_empty_space_5M_out, PPO_empty_space on 5M iterations with Xinyu hyperparameters; gamma=0.9, , Stopped
PPO_neighbor_search_random_walk_optimal_5M, ppo_neighbor_search_random_walk_5M_out, PPO_neighbor_search_random_walk on 5M iterations with Xinyu hyperparameters, , Stopped
PPO_random_search_random_walk_optimal_5M, ppo_random_search_random_walk_5M_out, PPO_random_search_random_walk on 5M iterations with Xinyu hyperparameters, , Stopped
PPO_empty_space_optimal_5M_2, ppo_empty_space_5M_2_out, PPO_empty_space on 5M iterations with Xinyu hyperparameters; gamma=0.5, , Stopped
PPO_normal_train_optimal_5M, ppo_normal_train_5M_out, PPO normal train on 5M iterations with Xinyu hyperparameters
----------------
AntDir-v0
----------------
PPO_1, ppo_init_1M_out, ppo_antdir_1M.zip; PPO Init for 1M steps with Xinyu hyperparameters
PPO_2, ppo_init_5M_out, ppo_antdir_5M.zip; PPO Init for 5M steps with Xinyu hyperparameters
----------------
PPO_empty_space_optimal_5M_1, ppo_empty_space_5M_out, PPO_empty_space on 5M iterations with Xinyu hyperparameters; gamma=0.9, , Stopped
PPO_empty_space_optimal_5M_2_1, ppo_empty_space_5M_2_out, PPO_empty_space on 5M iterations with Xinyu hyperparameters; gamma=0.5, , Stopped
PPO_empty_space_optimal_5M_3_1, ppo_empty_space_5M_3_out, PPO_empty_space on 5M iterations with Xinyu hyperparameters; gamma=0.3, , Stopped
PPO_neighbor_search_random_walk_optimal_5M_1, ppo_neighbor_search_random_walk_5M_out, PPO_neighbor_search_random_walk on 5M iterations with Xinyu hyperparameters, , Stopped
PPO_random_search_random_walk_optimal_5M_1, ppo_random_search_random_walk_5M_out, PPO_random_search_random_walk on 5M iterations with Xinyu hyperparameters, , Stopped
PPO_normal_train_optimal_5M, ppo_normal_train_5M_out, PPO normal train on 5M iterations with Xinyu hyperparameters
----------------
Ant-v5 Less Evaluations
----------------
PPO_empty_space_ls_1, ppo_empty_space_ls_out, PPO_empty_space with 100 iterations - 300k; gamma=0.9, , Stopped
PPO_empty_space_ls_2, ppo_empty_space_ls_2_out, PPO_empty_space with 60 iterations & 3 evaluation - 120k; gamma=0.9, , Stopped
PPO_empty_space_ls_3, ppo_empty_space_ls_3_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.9,
PPO_empty_space_ls_4, ppo_empty_space_ls_4_out, PPO_empty_space with 100 iterations & advantage estimation - 300k; gamma=0.9, , Stopped
PPO_empty_space_ls_5, ppo_empty_space_ls_5_out, PPO_empty_space with 60 iterations & 3 evaluation & advantage estimation - 120k; gamma=0.9, , Stopped
PPO_empty_space_ls_6, ppo_empty_space_ls_6_out, PPO_empty_space normal & advantage estimation - 1M; gamma=0.9, , Stopped
PPO_empty_space_ls_7, ppo_empty_space_ls_7_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.5
PPO_empty_space_ls_9, ppo_empty_space_ls_9_out, PPO_empty_space normal & advantage estimation for fqe v1- 1M; gamma=0.3, , Stopped
-----------------
Ant-v5
----------------
PPO_1, ppo_init_1M_out, ppo_ant_1M.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
PPO_2, ppo_init_5M_out, ppo_ant_5M.zip; PPO Init for 5M steps with Xinyu hyperparameters
PPO_empty_space_ls_8, ppo_empty_space_ls_8_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.3
PPO_empty_space_ls_11, ppo_empty_space_ls_11_out, Advantage estimation using top 5 agent and 2 evaluation & new fqe & search=1- 10k; gamma=0.3, , Stopped
PPO_empty_space_ls_12, ppo_empty_space_ls_12_out, Advantage estimation using top 5 agent & 2 evaluation & new fqe & search=2- 10k; gamma=0.3, , Stopped
PPO_empty_space_ls_13, ppo_empty_space_ls_13_out, Advantage estimation using top 5 agent & 2 evaluation & new fqe & search=3- 10k; gamma=0.3, , Stopped
PPO_baseline_1, ppo_baseline_1_out, Normal training with 10512 steps per rollout & batch_size=657; search=1
PPO_baseline_2, ppo_baseline_2_out, Normal training with 5512 steps per rollout & batch_size=345; search=2
PPO_baseline_3, ppo_baseline_3_out, Normal training with 3845 steps per rollout & batch_size=240; search=3
PPO_FQE_1, ppo_fqe_1_out, PPO FQE with 60 iterations & every other point; gamma=0.3
----------------
HalfCheetah-v5
----------------
PPO_1, ppo_init_1M_out, ppo_half_cheetah_1M.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
PPO_2, ppo_init_5M_out, ppo_half_cheetah_5M.zip; PPO Init for 5M steps with Xinyu hyperparameters
PPO_empty_space, ppo_empty_space_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.3
PPO_empty_space_ls_1, ppo_empty_space_ls_1_out, Advantage estimation using top 5 agent and 2 evaluation & new fqe & search=1- 10k; gamma=0.3, , Stopped
PPO_empty_space_ls_2, ppo_empty_space_ls_2_out, Advantage estimation using top 5 agent & 2 evaluation & new fqe & search=2- 10k; gamma=0.3, , Stopped
PPO_empty_space_ls_3, ppo_empty_space_ls_3_out, Advantage estimation using top 5 agent & 2 evaluation & new fqe & search=3- 10k; gamma=0.3, , Stopped
PPO_baseline_1, ppo_baseline_1_out, Normal training with 10512 steps per rollout & batch_size=657; search=1
PPO_baseline_2, ppo_baseline_2_out, Normal training with 5512 steps per rollout & batch_size=345; search=2
PPO_baseline_3, ppo_baseline_3_out, Normal training with 3845 steps per rollout & batch_size=240; search=3
----------------
Hopper-v5
----------------
PPO_1, ppo_init_1M_out, ppo_hopper_1M.zip; PPO Init for 1M steps with new hyperparameters
PPO_2, ppo_init_5M_out, ppo_hopper_5M.zip; PPO Init for 5M steps with new hyperparameters
PPO_empty_space, ppo_empty_space_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.3
PPO_empty_space_ls_1, ppo_empty_space_ls_1_out, Advantage estimation using top 5 agent and 2 evaluation & new fqe & search=2- 10k; gamma=0.3, , Stopped
PPO_baseline_1, ppo_baseline_1_out, Normal training with 5512 steps per rollout & batch_size=345; search=2
----------------
Walker2d-v5
----------------
PPO_1, ppo_init_1M_out, ppo_walker2d_1M.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
PPO_2, ppo_init_5M_out, ppo_walker2d_5M.zip; PPO Init for 5M steps with new hyperparameters
PPO_empty_space, ppo_empty_space_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.3
PPO_FQE_1, ppo_fqe_1_out, PPO FQE with 60 iterations & every other point - 60k; gamma=0.3, , Stopped
----------------
Humanoid-v5
----------------
PPO_1, ppo_init_1M_out, ppo_humanoid_1M.zip; PPO Init for 1M steps with new hyperparameters and replay buffer
PPO_2, ppo_init_5M_out, ppo_humanoid_5M.zip; PPO Init for 5M steps with new hyperparameters
PPO_empty_space_1, ppo_empty_space_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.3
PPO_empty_space_ls_1, ppo_empty_space_ls_1_out, Advantage estimation using top 5 agent and 2 evaluation & new fqe & search=1- 10k; gamma=0.3, , Stopped
PPO_empty_space_ls_2, ppo_empty_space_ls_2_out, Advantage estimation using top 5 agent & 2 evaluation & new fqe & search=2- 10k; gamma=0.3, , Stopped
PPO_empty_space_ls_3, ppo_empty_space_ls_3_out, Advantage estimation using top 5 agent & 2 evaluation & new fqe & search=3- 10k; gamma=0.3, , Stopped
PPO_baseline_1, ppo_baseline_1_out, Normal training with 10512 steps per rollout & batch_size=657; search=1
PPO_baseline_2, ppo_baseline_2_out, Normal training with 5512 steps per rollout & batch_size=345; search=2
PPO_baseline_3, ppo_baseline_3_out, Normal training with 3845 steps per rollout & batch_size=240; search=3
-----------------
Swimmer-v5
-----------------
PPO_1, ppo_init_1M_out, ppo_swimmer_1M.zip; PPO Init for 1M steps with new hyperparameters and replay buffer
PPO_FQE_1, ppo_fqe_1_out, PPO FQE with 60 iterations & every other point - 60k; gamma=0.3, , Stopped
----------------
CartPole-v1
----------------
PPO_1, ppo_init_1M_out, ppo_cartpole_1M.zip; PPO Init for 1M steps with new hyperparameters
PPO_2, ppo_init_3M_out, ppo_cartpole_3M.zip; PPO Init for 3M steps with new hyperparameters
PPO_empty_space_1, ppo_empty_space_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.3, , //Exited due to error
----------------
MountainCar-v0
----------------
PPO_1, ppo_init_1M_out, ppo_mountaincar_1M.zip; PPO Init for 1M steps with new hyperparameters
PPO_2, ppo_init_3M_out, ppo_mountaincar_3M.zip; PPO Init for 3M steps with new hyperparameters
PPO_empty_space_1, ppo_empty_space_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.3, , //Need to run
----------------
Pendulum-v1
----------------
PPO_1, ppo_init_1M_out, ppo_pendulum_1M.zip; PPO Init for 1M steps with new hyperparameters
PPO_2, ppo_init_3M_out, ppo_pendulum_3M.zip; PPO Init for 3M steps with new hyperparameters
PPO_empty_space_1, ppo_empty_space_out, PPO_empty_space with 60 iterations & 3 evaluation & every other point - 60k; gamma=0.3
PPO_baseline_1, ppo_baseline_1_out, Normal training with 11024 steps per rollout & batch_size=689; search=1


#####################################################################################################################################################################

------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------
Ant-v5
------------------------------------------------------------------------------------------------
Seed 0         |
----------------
PPO_1, ppo_init_1M_out, ppo_ant_1M_0.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
PPO_FQE_1, ppo_fqe_1_out, PPO FQE with 60 iterations & every other point - 60k; gamma=0.3, espace1, Ongoing
----------------
Seed 1         |
----------------
PPO_2, ppo_init_1M_1_out, ppo_ant_1M_1.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
PPO_FQE_2, ppo_fqe_2_out, PPO FQE with 60 iterations & every other point - 60k; gamma=0.3, espace6, Ongoing
----------------
Seed 2         |
----------------
PPO_3, ppo_init_1M_2_out, ppo_ant_1M_2.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------
HalfCheetah-v5
------------------------------------------------------------------------------------------------
Seed 0         |
----------------
PPO_1, ppo_init_1M_out, ppo_half_cheetah_1M_0.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
PPO_FQE_1, ppo_fqe_1_out, PPO FQE with 60 iterations & every other point - 60k; gamma=0.3, espace2, Ongoing
----------------
Seed 1         |
----------------
PPO_2, ppo_init_1M_1_out, ppo_half_cheetah_1M_1.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
----------------
Seed 2         |
----------------
PPO_3, ppo_init_1M_2_out, ppo_half_cheetah_1M_2.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------
Walker2d-v5
------------------------------------------------------------------------------------------------
Seed 0         |
----------------
PPO_1, ppo_init_1M_out, ppo_walker2d_1M_0.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
PPO_FQE_1, ppo_fqe_1_out, PPO FQE with 60 iterations & every other point - 60k; gamma=0.3, espace3, Ongoing
----------------
Seed 1         |
----------------
PPO_2, ppo_init_1M_1_out, ppo_walker2d_1M_1.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer
----------------
Seed 2         |
----------------
PPO_3, ppo_init_1M_2_out, ppo_walker2d_1M_2.zip; PPO Init for 1M steps with Xinyu hyperparameters and replay buffer, espace8, Ongoing
------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------
Humanoid-v5
------------------------------------------------------------------------------------------------
Seed 0         |
----------------
PPO_1, ppo_init_1M_out, ppo_humanoid_1M_0.zip; PPO Init for 1M steps with new hyperparameters and replay buffer
PPO_FQE_1, ppo_fqe_1_out, PPO FQE with 60 iterations & every other point - 60k; gamma=0.3, espace4, Ongoing
----------------
Seed 1         |
----------------
PPO_2, ppo_init_1M_1_out, ppo_humanoid_1M_1.zip; PPO Init for 1M steps with new hyperparameters and replay buffer
----------------
Seed 2         |
----------------
PPO_3, ppo_init_1M_2_out, ppo_humanoid_1M_2.zip; PPO Init for 1M steps with new hyperparameters and replay buffer, espace11, Ongoing
------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------
Swimmer-v5
------------------------------------------------------------------------------------------------
Seed 0         |
----------------
PPO_1, ppo_init_1M_out, ppo_swimmer_1M_0.zip; PPO Init for 1M steps with new hyperparameters and replay buffer
PPO_FQE_1, ppo_fqe_1_out, PPO FQE with 60 iterations & every other point - 60k; gamma=0.3, espace5, Ongoing
----------------
Seed 1         |
----------------
PPO_2, ppo_init_1M_1_out, ppo_swimmer_1M_1.zip; PPO Init for 1M steps with new hyperparameters and replay buffer
PPO_FQE_2, ppo_fqe_2_out, PPO FQE with 60 iterations & every other point - 60k; gamma=0.3, espace10, Ongoing
----------------
Seed 2         |
----------------
PPO_3, ppo_init_1M_2_out, ppo_swimmer_1M_2.zip; PPO Init for 1M steps with new hyperparameters and replay buffer